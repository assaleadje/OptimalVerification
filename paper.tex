\documentclass[10pt]{llncs}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{amsthm}
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{version}
\usepackage{url}

%\allowdisplaybreaks
 \newcommand\ForAuthors[1]%          %  temporary remark for the
 {\par\smallskip                     %  authors:
  \begin{center}%                    %
   \fbox%                            %    --------
   {\parbox{0.9\linewidth}%          %    |  #1  |
    {\raggedright\sc--- #1}%         %    --------
   }%                                %
  \end{center}%                      %
  \par\smallskip                     %
 }
%\begin{document}
\input{raccourcis}

\title{Optimal Verification of Discrete-time Affine Systems}
%\author{Assalé Adjé\footnote{The author was partially supported by the PGMO program of EDF and Fondation Math\'{e}matique Jacques Hadamard. }\\
%Université of Perpignan Via Domitia\\
%LAMPS\\
%France\\
%\url{assale.adje@univ-perp.fr}
%}
%\date{}%

\begin{document}
\maketitle
\begin{abstract}
 Our very first concern is the resolution of the verification problem for the class of discrete-time affine dynamical systems. This verification problem is turned into an optimization problem where the constraint set is the reachable values set of the dynamical system. To solve this optimization problem, we truncate the infinite traces at some rank which is uniform with respect to the initial conditions. In theory, the best possible uniform rank is the optimal solution of a non-convex semi-definite program. In practice,  we propose a methodology to compute a uniform rank that over-approximate the best solution.
\end{abstract}

\section{Introduction}
\paragraph{Motivations.}
Some catastrophic events~\cite{johnson2005natural,mcquaid2012software} caused by bugs in programs explain the importance of the formal verification of programs in general and the static analysis~\cite{cousot2010gentle} in particular. The interest is shared by both theorists and industrials. Indeed, since the success of Astrée~\cite{delmas2007astree,souyris2007experimental,bouissou2009space}, industrials are  more and more concerned with static analysis.  To solve static analysis problems is thus an interesting scientific challenge for the theory as well as for the applications and the pratical  side.   

This interest for static analysis techniques comes from its exhaustivity. Indeed static analysis offers a deeper analysis compared to traditional test techniques which can't cover all possible situations. The principle of static analysis consists in proving properties automatically on programs without executing them. Static analyzers only use the structures of programs not the values generated by the program.  Numerous various techniques exist but none of them can't be complete. This means that no program can validate or disprove a given property for all programs. Static analysis is not only useful for programs but can also be applied to validate properties on dynamical systems in general. The principle is still the same : to prove properties on dynamical systems without any simulations.

To prove or disprove property even for discrete-time linear system is still an open challenge~\cite{acceleration-popl14,7403149,DBLP:conf/icalp/AlmagorCO018}.  The difficulty of the analysis lies in the infinite number of points in the reachable values set. Static analysis abstract interpretation~\cite{cousot2001abstract} based proposes to prove properties by means of over-approximations of the reachable values set. But abstract interpretation has difficulties to disprove a property since over-approximations can contain non-reachable values which violate the property.

The main motivation of the paper is to propose a technique that can be, for a given discrete-time affine system and a given property, to prove or disprove the property on the system.    

\paragraph{Related works.} 
The paper solves a formal verification problem. Numerous works (for example~\cite{DBLP:journals/corr/abs-1111-5223,gonnord2014abstract,mine-ESOP16,Sankaranarayanan_BenSassi__2017__Template}) about the resolution of the problem by abstract interpretation are available. They are mainly concerned in the representation of the reachable values. The over-approximations produced are not sufficient to solve a specific verification problem with a given property.

Hence, we only consider related works which produce a proof for a given property. Moreover, we are focused on close techniques based on optimization theory and/ or Lyapunov functions to solve the verification problem. 

Proof techniques based on Lyapunov functions~\cite{roozbehani2013optimization,blanchini2009lyapunov} automatically deals with non-convexities. This makes difficult the exact resolution of the verification problem. To use Lyapunov functions to solve verification problems leads  to over-approximations and the only conclusion that can be made is "the property holds or we do not know".

In~\cite{adje2015property}, the authors propose  an approach based on Sums-of-Squares optimization to prove property on discrete-time polynomial systems. The verification problem is reduced to an optimization problem exactly the same that we use here.  The objective function comes from the property to prove. The proof of the property requires the synthesis of a polynomial invariant i.e.  a sublevel set of a polynomial that over-approximates the reachable value set of the polynomial system. The computed sublevel is the one that minimizes the over-approximation. This approach uses abstract interpretation and then cannot disprove a property. Actually, the main issue is that we cannot prove that there is a "zero duality gap result" . The same kind of approach exists for piecewise affine systems~\cite{10.1007/978-3-319-54292-8_2} with the same disadvantages. 

The closest work seems to be the work of Ahmadi et al~\cite{7403149,ahmadi2018robust} which introduces the class of robust-to-dynamics optimization problem. Ahmadi et al solve the problem where the dynamics and the objective function are linear. Moreover, they need a bounded invariant set to solve their problem. In our work, we deal with non-linear quadratic  forms and affine dynamical systems.  Moreover, we do not need a bounded invariant to solve our robust-to-dynamics optimization problem.  Nevertheless, some similarities exist between their approach and the one proposed here : the use of a solution of the discrete Lyapunov matrix equation and the computation of a number of steps to convergence.   
\paragraph{Contributions.}
Our very first concern is the resolution of the verification problem for the class of discrete-time affine dynamical systems. We assume that the given property is written as the sublevel set of some quadratic function. 
We reduced the verification problem to an optimization problem :
\begin{equation}
\label{eqintro}
\sup\{f(x)\mid x_{k+1}=g(x_k),\ x_0\in\xin\}=\sup\{f( g^k (x_0))\mid  x_0\in\xin,\ k\in\nn\}\enspace.
\end{equation}
where $f$ is a quadratic function, $g$ an affine function and $\xin$ is a polytope. We remark that the optimal value in~\eqref{eqintro} only depends on the initial values.
%\[
%\sup\{f( g^k (x_0))\mid  x_0\in\xin,\ k\in\nn\}\enspace.
%\]
In the paper, we compute a number of steps $\bigk$ after which the search of the optimal value is useless i.e.
 \begin{equation}
\label{eqintro2}
\sup\{f( g^k (x_0))\mid  x_0\in\xin,\ k\in\nn\}=
\sup\{f( g^k (x_0))\mid  x_0\in\xin,\ k\in \{0,\ldots,\bigk\}\}
\end{equation}
This number of steps $\bigk$ is valid for all possible initial values. We are looking for the best possible integer $\bigk$ for which 
Eq.~\eqref{eqintro2} is true. We propose to construct a family of integers parameterized by some positive reals and some matrices. The approach proposed in the paper uses basic tools from matrix theory such as the discrete Lyapunov matrix equation, minimal and maximal eigenvalues, matrix norms and generalized Raleigh quotients. In theory, the best proposed integer is then the optimal value of some non-convex semi-definite program.  
The exact resolution of this semi-definite problem is, for the moment, left open and we just provide an over-approximation of its optimal value which does not degrade the exact resolution of the verification problem.   For the moment, we propose in practice the computation of two families of integers which over-approximate the best value. 

%\vspace{-0.4cm}

\paragraph{Organization of the paper.}
Section~\ref{statement} states the problem i.e. the formulation of our verification problem and its translation into a optimization problem. Section~\ref{mainresults} presents the main results. We give details about the construction of an integer solution of Eq.~\eqref{eqintro2}. Section~\ref{computations} describes the practical idea used in the implementation.  Section~\ref{experiments} is devoted to the experimentations. Finally, Section~\ref{conclusion} concludes and opens future research directions.

%\vspace{-0.4cm}

\paragraph{Notations.}
%In this paper, we denote by $\Idd$ the identity matrix of size $d\times d$. We denote, for a matrix $M$, by $M^\intercal$ the transpose of $M$. The set of the (square) symmetric matrices  (the matrices $M$ such that $M=M^\intercal$) of size $d$ is denoted by $\sym$.
In this paper, we denote by $\Idd$ the identity matrix of size $d$.
We also denote, for a matrix $M$, by $M^\intercal$ the transpose of $M$. The matrix $M$ is {\it symmetric} iff $M=M^\intercal$.

We recall that a matrix $M$ of size $d\times d$ is {\it positive semidefinite} (resp. {\it positive definite}) iff $M$ is symmetric and for all $x\in\rd$, $x^\intercal M x\geq 0$ (resp. $M$ is symmetric and for all $x\in\rd$, $x\neq 0$, $x^\intercal M x> 0$). We write $M\succeq 0$ (resp. $M\succ 0$)  when $M$ is positive semi-definite (resp. positive definite).
%{\it positive semidefinite} iff $M$ is symmetric for all $x\in\rd$,
%$x^\intercal M x\geq 0$. Whereas the matrix $M$ is said to positive definite if and only if $M$ is symmetric for all $x\in\rd$, $x\neq 0$, 
%$x^\intercal M x>0$. 

An equivalent definition relies on eigenvalues i.e., for a square matric $M$, the values $\lambda\in\mathbb C$ s. t. $Mx=\lambda x$ for some non zero $x$. Then $M\succeq 0$ (resp. $M\succ 0$) iff $M$ is symmetric and all its eigenvalues are non-negative (resp. positive); recalling that the eigenvalues of a symmetric matrix are real.

For a symmetric matrix $M$, we denote by $\lmax{M}$ (resp. $\lmin{M}$) the greatest (resp. smallest) eigenvalue of $M$.
%Finally, a matrix $M$ is negative semidefinite (resp. definite) if $-M$ is positive semidefinite (resp. definite).

%For a given symmetric matrix $M$ of size $d$ , we will consider the real eigenvalues in descending order   $\lmax{M}=\lambda_1(M)\geq \lambda_2(M)\geq \ldots\geq \lambda_d(M)=\lmin{M}$. 

Finally, we denote by $\rho(M)$ the spectral radius of a square matrix $M$ of size $d$ that is the quantity $\max\{|\lambda_i(M)|,\ i=1,\ldots,d\}$ where $|\cdot|$ denotes the modulus of a complex number.

%\vspace{-0.5cm}

\section{Problem statement}
\label{statement}
In this paper, we are interested in proving automatically some properties on a discrete-time affine system. We recall briefly the notion of affine systems and their reachable values set. Then we present the optimization problem that we have to solve to prove or disprove a property on an affine system.
\subsection{Affine systems}
We can represent the evolution of an affine system by the following relation:
\begin{equation}
\label{context}
x_0\in \xin,\ \forall\, k\in\nn,\ x_{k+1}=A x_k + b  
\end{equation}
where $A$ is a non-zero square matrix of size $d\times d$; $b$ is vector of $\rd$; $\xin$ is a non-empty polytope (bounded polyhedra) non reduced to one vector. 

We insist on the fact that the initial values are represented by an infinite bounded set. It can be interpreted as a set of perturbations, non-determinism, different cases...

It is well-known that, for all $k\in\nn$, the term of the sequence defined at Eq.~\eqref{context} can be expressed as follows:
\[
x_k=A^k x_0+ \sum_{i=0}^{k-1} A^i b,\ x_0\in\xin
\]

From the latter expression of the state-variables, we can define the reachable values set $\rea$ of the affine system presented at Eq.~\eqref{context} :
\begin{equation}
\label{reach}
\rea=\bigcup_{k\in \nn} \left(A^{k}(\xin)+\sum_{i=0}^{k-1} A^i b\right)
\end{equation}
where $A^i$ denotes the number of image iterates of $A$ (powers of matrix $A$). 

\subsection{The verification problem}
In this paper, we are focusing on the properties supposed to be true for all possible values of the state-variable $x_k$ i.e. for all $k\in\nn$, $x_k$ has to satisfy the property. Since, we can represent a property as to belong to some set $C\subset \rd$, to prove a property is equivalent to prove that, $\rea\subseteq C$. Here, we consider sets $C$  of the form $\{x\in\rd, x^\intercal Q x+q^\intercal x\leq \alpha\}$ where $Q$ is symmetric and $q\in\rd$. The real number $\alpha$ can be given or proved to be finite (for example to prove the boundedness).  Thus, a verification problem can be viewed as an optimization problem. Indeed, $\rea\subseteq \{x\in\rd, x^\intercal Q x+q^\intercal x\leq \alpha\}$ is equivalent to
$\sup_{x\in\rea} x^\intercal Q x+q^\intercal x\leq \alpha$.
Then the verification problem boils down to compute: 
\begin{equation}
\label{pbopt}
\begin{array}{ll}
&\displaystyle{\sup_{x\in\rea} x^\intercal Q x+q^\intercal x}\\
=&\displaystyle{\sup_{k\in\nn}\sup_{x_0\in\xin}  \left(A^k x_0+ \sum_{i=0}^{k-1} A^i b \right) Q \left( A^k x_0+ \sum_{i=0}^{k-1} A^i b\right) +q^\intercal \left(A^k x_0+ \sum_{i=0}^{k-1} A^i b\right)}\enspace .
\end{array}
\end{equation} 

If the exact optimal value of Problem~\eqref{pbopt} can be computed then we can prove or disprove a property whereas an over-approximation can only prove a property: a too loose over-approximation i.e. greater than $\alpha$ does not imply that the property is false. 

The problem in the computation of the optimal value of Problem~\eqref{pbopt} resides, first, in the infinite number of quadratic optimization problems that we have to solve. In a second time, the quadratic optimization problems are non-necessarily concave and thus can be difficult to solve. 
\section{Main Results}
\label{mainresults}
This section contains the main contribution of the paper i.e. the computation of the optimal value of Problem~\eqref{pbopt}. This latter computation uses a discretization of initial values set and the computation of a finite integer after which we know that the optimal cannot be reached. In this section, we begin to recall some basic results that we need in our development. We then study the case of linear systems i.e. where $b=0$ in Eq.~\eqref{context}. Finally, we show how to use the linear case to solve the case where the system is affine.   
\subsection{Basic notions}

For a given square matrix $M$, the matrix $P$ satisfies the discrete Lyapunov equation iff $P\succ 0$ and $P-M^\intercal P M\succeq 0$. If $\rho(M)<1$, then the discrete Lyapunov equation has at least one solution. More precisely, $\rho(A)<1$ iff for all $R\succ 0$, there exists a unique $P\succ 0$ such that $P-A^\intercal P A=R$. 

For the matrix $A$ of the affine system considered at Eq.~\eqref{context}, we introduce the set $\lyap{A}$ of the  solutions  of the discrete Lyapunov equation :
\[
\lyap{A}=\{P\succ 0 \mid\ P-A^\intercal P A\succ 0\}\enspace .
\]
%Hence, $\rho(A)<1$ ensures that $\lyap{A}$ is non-empty. More precisely, $\rho(A)<1$ if and only if for all positive definite matrices $R$ there exists a unique positive definite matrix $P$ such that $P-A^\intercal P A=R$.

%To prove some results, we need to recall some basic results presented as lemmas. The first one involves a double inequality between quadratic forms and Euclidean norm. When the quadratic form is positive definite then the double inequality proves the norm equivalence between the norm defined by the quadratic form and the Euclidean norm with explicit constants. 
%\begin{lemma}
%\label{lemma1}
%Let $M$ be a symmetric matrix of size $d$. 
%%We recall that $\lmax{M}$ (resp. $\lmin{M}$) denotes the greatest eigenvalue of $M$ (resp. the smallest eigenvalue of $M$) 
%Then, for all $x\in \rd$:
%\begin{equation}
%\label{eigineg}
%\lmin{M}\norm{x}_2^2\leq x^\intercal M x\leq \lmax{M} \norm{x}_2^2
%\end{equation}
%where $\norm{\cdot}_2$ is the Euclidean norm. 
%\end{lemma}
%Inequalities~\eqref{eigineg} can be formulated in term of positive semi-definiteness:
%\[
%\lmin{M}\Idd\preceq M\preceq \lmax{M}\Idd
%\]

%We recall Weyl's inequalities~\cite{horn1990matrix} that provide inequalities for the eigenvalues of the sum of two symmetric matrices and the sum of the eigenvalues of each matrix.  
%\begin{lemma}[Weyl's inequalities]
%\label{lemmaWeyl}
%Let $M$ and $N$ be symmetric matrices. We have, for all $\ell\in\{1,\ldots,d\}$ :
%\[
%\lambda_\ell(M)+\lmin{N}\leq \lambda_\ell(M+N)\leq \lambda_\ell(M)+\lmax{N}
%\]
%\end{lemma}
The following lemma recalls that, when we maximize a convex function over a polytope, it suffices to consider the values of the function at the extreme points. 
\begin{lemma}[Maximisation of a convex function over a polytope]
\label{lemma2}
Let $C$ be a polytope and $f:\rd\to\rd$ a convex function. Then:
\[
\max_{x\in C} f(x)=\max_{x\in \mathcal{E}(C)} f(x)
\]
where $\mathcal{E}(C)$ denotes the finite set of the extreme points (vertices) of $C$.
\end{lemma}
Note that the result also holds when $C$ is a convex compact set.  
\subsection{Verification of Linear Discrete-time Systems}
\label{mainsub}
Now we come back to the verification problem presented at Equation~\eqref{pbopt}. In this subsection, we suppose that the system in linear i.e. $b=0$ in~\eqref{context}. We will show at Subsection~\ref{affine} that the case $b\neq 0$ can be reduced to the linear case.

Since $b=0$, Problem~\eqref{pbopt} becomes :
\begin{equation}
\label{pblinear}
\sup_{k\in\nn}\sup_{x_0\in\xin} x_0^\intercal (A^k)^\intercal Q A^k x_0+q^\intercal A^k x_0\enspace .
\end{equation}

The main goal of this subsection is to {\bf compute the smallest possible integer} $\mathbf K$ such that:
\[
 \sup_{x\in\rea} x^\intercal Q x+p^\intercal x=\sup_{k\in\{0,\ldots,\mathbf K\}}\sup_{x_0\in\xin} x_0^\intercal (A^k)^\intercal Q A^k x_0+q^\intercal A^k x_0\enspace .
\]

Without any assumption on $Q$, the computation of  $\sup_{x_0\in\xin} x_0^\intercal (A^k)^\intercal Q A^k x_0+q^\intercal A^k x_0$ for all $k\in\{0,\ldots,K\}$ is still difficult. Then we make the following assumption : 
\begin{assumption}
\label{assum1}
The matrix $Q$ is non-null and positive semi-definite.
\end{assumption}

From Assumption~\ref{assum1}, Problem~\eqref{pbopt} has a finite optimal value if either $\rea$ is bounded or for all 
$x_0\in\xin$ that generates an unbounded sequence, the sequence $x_k=A^k x_0$ lies eventually in the null space of $Q$ ($x_k^\intercal Q x_k=0$ for sufficiently large integers $k$) and satisfies eventually $q^\intercal A^k x_0\leq \beta$ for some $\beta\in\rr$ .  We will consider bounded reachable values set. This latter restriction will not be enough for our development. Using Jordan block decompositions, the reachable values set can be bounded even if the spectral radius of $A$ is equal to 1. The construction of our $K$ depends on the fact that there exists a matrix norm $N$ such that $N(A)<1$. This condition is not compatible with the case where the spectral radius of $A$ is equal to 1.

\begin{assumption}
\label{assum2}
The spectral radius $\rho(A)$ of $A$ is strictly smaller that 1.
\end{assumption}

Assumption~\ref{assum2} ensures that $\lyap{A}\neq \emptyset$. Since for all $P\in\lyap{A}$, $P\succ 0$, we can define a norm on $\rd$ associated with $P\in\lyap{A}$ as follows: for all $x\in\rd$:
\[\norm{x}_P:=\sqrt{x^\intercal P x}\enspace. \]
For any norm $\norm{\cdot}$, we can define the operator norm as $\norm{A}=\max_{x\neq 0} x^\intercal A x/ x^\intercal x$. In the case where the norm is constructed from a positive definite matrix, we can compute $\norm{A}_P$ as follows:
\begin{equation}
\label{normdef}
    \norm{A}_P^2=\max_{x\neq 0} \dfrac{x^\intercal A^\intercal P A x}{x^\intercal P x}=\inf\{\alpha>0\mid \alpha P-A^\intercal P A\succeq 0\}
\end{equation}

%Let $t>0$. We introduce the following set :  
%\[
%\conq(t)=\{P\in\sym\mid tP-Q\succeq 0\}
%\]
%Since, under Assumption~\ref{assum1}, $P\in\conq(t)$ implies that $P$ is positive semi-definite, then $\conq(t)$ is a pointed convex cone. Moreover, for all positive definite matrix $P$, there exists $t>0$ such that $P\in \conq(t)$. 

%Then, we introduce a related set, which is 
We introduce the following set parameterized by a symmetric matrix $P$.
\[
\cont(P)=\{t>0\mid tP-Q\succeq 0\}
\]

Let us introduce the family $\lyat{A,Q}$ parameterized by positive numbers $t$ of sets of solutions of discrete Lyapunov equations defined as follows:
\[
\lyat{A,Q}=\{P\succ 0\mid tP-Q\succeq 0,\ P-A^\intercal P A\succ 0\}\enspace. 
\]
Note that, since the set of positive definite matrix is a cone, we get the following equivalence :
\begin{equation}
\label{simpleequiv}
P\in\lyat{A,Q}\iff tP\in\lyaun\enspace.
\end{equation}
 
Actually we can bound $\cont(P)$ from below. Since $P$ is positive definite, we can use the inverse of the square root $P^{-1/2}$ of $P$ . We recall that this matrix is defined from the spectral decomposition of $P=U D U^\intercal$ where $D$ is a diagonal  matrix of ordered (strictly positive) eigenvalues $\lambda_k(P)$ and $U$ is an orthogonal matrix i.e. $U U^\intercal = U^\intercal U=\Idd$.   The matrix  $P^{-1/2}$ is thus defined by 
$U D^{-1/2} U^\intercal$ where $D^{-1/2}$ is the diagonal matrix whose the diagonal elements are equal $\lambda_k(P)^{-1/2}$. This easy to see that $P^{-1/2} P^{-1/2}=P^{-1}$. 

\begin{prop}
\label{propcont}
For all $P\succ 0$, for all $t\in\cont(P)$, $t\geq \lmax{P^{-1/2} Q P^{-1/2}}$.
\end{prop}
\begin{proof}
In appendix.
\end{proof}
%\begin{proof}
%Let $P\succ 0$, let $t\in\cont(P)$. Let $x\in\rd$, $x\neq 0$. We have 
%$x^\intercal Q x\leq t x^\intercal P x$ and thus $(x^\intercal Q x)(x^\intercal P x)^{-1}\leq t$. Finally:
%$\sup_{x\neq 0}(x^\intercal Q x)(x^\intercal P x)^{-1}\leq t $. Writing $y=P^{1/2}x$ ( recall that $P^{1/2}$ is invertible as $P\succ 0$), we obtain :
%$\sup_{y\neq 0}(y^\intercal P^{-1/2}Q P^{-1/2}y)(y^\intercal y)^{-1}\leq t $. However, it is well-know that
%$\sup_{z\neq 0} (z^\intercal M z)(z^\intercal z)^{-1}=\lmax{M}$. In fact, $\lmax{M}$ is achieved for an eigenvector associated with $\lmax{M}$. Hence, 
%$\sup_{y\neq 0} (y^\intercal P^{-1/2}Q P^{-1/2}y)(y^\intercal y)^{-1}=\lmax{P^{-1/2} Q P^{-1/2}}$. Finally $\lmax{P^{-1/2} Q P^{-1/2}}P-Q\succeq 0$ is a direct consequence of Lemma~\ref{lemma1}. 
%
%\end{proof}
To compute our $\bigk$, we introduce a new notation and a new assumption. 
 \begin{assumption}
\label{assumR}
There exists $k\in\nn$ such that\[\min\left\{\sup_{x\in\xin} x^\intercal Q x , \sup_{x\in\xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\right\}\] is strictly positive.
\end{assumption}

Note that Assumption~\ref{assumR} implies that $\sup_{x\in\nn} x^\intercal Q x$ cannot be null. Even if for some $k$, the value
$\sup_{x\in\xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x$ is strictly positive, the minimal value would be equal to 0. 
Assumption~\ref{assumR} can be written as there exists  some $k\in\nn$, for which $\sup_{x\in\xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x$ is strictly positive and $\sup_{x\in\xin} x^\intercal Q x$ is strictly positive.
 
We will need the following notation :
\[
\ks:=\inf\left\{k\in\nn\mid \min\left\{\sup_{x\in\xin} x^\intercal Q x , \sup_{x\in\xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\right\}>0\right\}
\]
and
\[
\lfrak:= \min\left\{\sup_{x\in\xin} x^\intercal Q x , \sup_{x\in\xin} x^\intercal (A^{\ks})^\intercal Q A^{\ks} x+q^\intercal A^{\ks}x\right\}
\]

Note that Assumption~\ref{assum1} is not sufficient to ensure the validity of Assumption~\ref{assumR}. The matrix $Q$ is only supposed to be positive semi-definite and thus $\sup_{x\in\xin} x^\intercal Q x$ can be null. Moreover, $\sup_{x\in\xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x$ can be negative since $q^\intercal A^k x$ can be negative on $\xin$. Strongest assumptions can be made : the global positivity of $x^\intercal (A^k)^\intercal  Q A^k x+q^\intercal A^k x$ which is equivalent to check whether a matrix is definite positive or a copositivity condition relying $Q,q$ and $\xin$. 

\begin{prop}
\label{interiorprop}
If $\xin$ contains $0$ in its interior then Assumption~\ref{assumR} holds. In this case, $\ks=0$.
\end{prop}

%\begin{proof}
%Suppose that $q\neq 0$. Since 0 belongs to the interior of $\xin$, then it exists $\gamma>0$ such that $[-\gamma,\gamma]^d\subset \xin$. Let $x=\gamma q (2\norm{q}_{\infty})^{-1}$. Then $q^\intercal x=\norm{q}_2^2\norm{q}_{\infty}^{-1} >0$ and $x\in [-\gamma,\gamma]^d$. From $Q\succeq 0$, $x^\intercal Q x+q^\intercal x>0$. We thus have $\sup_{y\in \xin}  y^\intercal Q y+q^\intercal y>0$. 
%
%Now, since $Q$ is not null, there exists $z\in\rd$ such that $z\neq 0$ and $z^\intercal Q z>0$. Taking $x=\gamma z (2\norm{z}_\infty)^{-1}$, we get $x^\intercal Q x=\gamma ^2 (2\norm{z}_\infty)^{-2} z^\intercal Q z>0$ and $x\in[-\gamma,\gamma]^d$. Then $\sup_{y\in\xin} y^\intercal Q y>0$. 
%
%Finally, $\displaystyle{\min\left\{\sup_{x\in\xin} x^\intercal Q x , \sup_{x\in\xin} x^\intercal Q x+q^\intercal x\right\}>0}$ and $\ks=0$.
%\end{proof}

Prop.~\ref{interiorprop} uses the fact that there exists some $k\in\nn$ and some $x\in\xin$ for which all coordinates of $A^k x$ have the same sign as the ones of $q$. A more general proposition can be written using this fact. However, the existence of such $k$ and such $x$ cannot be decided.     
 
\begin{remark}
\label{remarklinear}
We can apply our method when the property is defined from linear forms. 
Let us consider the following special case. Let $\alpha,\beta\in\rr$ such that $\alpha<\beta$, and $c\in\rd$. The set $\{x\in\rd \mid \alpha \leq c^\intercal x\leq \beta\}$ can be rewritten as:
$x^\intercal (c c^\intercal) x-(\beta+\alpha) c^\intercal x \leq -\alpha \beta$. Then we have in this case $Q=c c^\intercal$ and $q= -(\beta+\alpha) c$. With these data, if Assumption~\ref{assumR} holds then $\alpha$ is negative and $\beta$ is positive. 
\end{remark}

Now, we define the key numbers that we need to solve the verification problem. We explain latter in the proofs how they are constructed. Moreover, we will study later on numerical examples how much they are accurate.  

Let $t>0$ and $P\in\lyap{A}$ such that $P\in \lyat{A,Q}$. Let us define the following formula :
\begin{equation}
\label{intgfond}
K(t,P)=\left\lfloor \dfrac{\ln\left[\left(\sqr{\lfrak+\vqQ(t,P)^2}-\vqQ(t,P)\right)\left(\sqr{t}\mu(P)\right)^{-1}\right]}{\ln\left(\norm{A}_P\right)}\right\rfloor+1
\end{equation}
where 
\[
\vqQ(t,P):=\dfrac{\norm{q}_2 }{2\sqr{t\lmin{P}}}\quad \text{ and } \mu(P)=\sup_{x\in\xin} \sqr{x^\intercal P x}
\]

First, we prove the consistency of the integers $K(t,P)$ with respect to Lyapunov functions. If $Q\in\lyap{A}$, then the sequence $\left(\sup_{x\in\xin} x^\intercal {A^k}^\intercal Q A^k x\right)_{k\in\nn}$ is strictly decreasing. Then if $q=0$, the optimal value of Problem~\eqref{pblinear} is achieved at $k=0$. 
\begin{prop}
\label{lyapunovbigk}
Suppose that $q=0$ and $Q\in\lyap{A}$ then $K(1,Q)=1$.  
\end{prop}

%\begin{proof}
%Since $Q\in\lyap{A}$ then $Q\succ 0$ and thus $\displaystyle{\sup_{x\in\xin} x^\intercal Q x>0}$ and $\ks=0$. We have $\lfrak=\displaystyle{\sup_{x\in\xin} x^\intercal Q x}$. Hence, since 
%$\sqrt{\cdot}$ is increasing then $\sqrt{\lfrak}=\displaystyle{\sup_{x\in\xin} \sqrt{x^\intercal Q x}}$. Finally, $K(1,Q)=1+\displaystyle{\ln\left(\sqrt{\lfrak}/\sqrt{\lfrak}\right) / \ln(\norm{A}_Q)}=1$.
%\end{proof}

Now, we prove that our family of integers is well-defined and is strictly positive.

\begin{prop}
For all $t>0$, $P\in \lyap{A}$ s. t. $P\in \lyat{A,Q}$, $1\leq K(t,P)< +\infty$.  
\end{prop}

It suffices to show that both numerator and denominator are negative and finite. Since we use the natural logarithm, this is equivalent to show that the arguments of the natural logarithm of the numerators lie in the interval $(0,1]$ and $\norm{A}_P$ lies in the interval $(0,1)$ for all $P\in\lyap{A}$. 

\begin{lemma}
\label{lyapnorm}
For all $P\in\lyap{A}$, $0<\norm{A}_P<1$. 
\end{lemma}
\begin{proof}
In appendix.
\end{proof}
%\begin{proof}
%Let $P$ such that $P\succ 0$ and $P-A^\intercal P A\succ 0$.  First, the matrix $A$ is not null then its spectral radius is strictly positive. Then since $\norm{\cdot}_P$ is a matrix norm, $0<\rho(A) \leq \norm{A}_P$.
%
%Secondly, since $P-A^\intercal P A\succ 0$ and using Eq.~\eqref{normdef}, we have $\norm{A}_P\leq 1$. Still using Eq.~\eqref{normdef}, to prove $\norm{A}_P<1$, it suffices to exhibit $\varepsilon>0$ such that $(1-\varepsilon) P-A^\intercal P A\succeq 0$ with $0<\varepsilon<1$. 
%
%Let us write $\varepsilon=\lmin{P-A^\intercal P A} \lmax{P}^{-1}$. Let us prove that $0<\varepsilon<1$ and $(1-\varepsilon) P-A^\intercal P A\succeq 0$.  Since $P-A^\intercal P A\succ 0$, $\lmin{P-A^\intercal P A}>0$ and from $P\succ 0$ we conclude that $\varepsilon >0$. Since $P=P-A^ \intercal P A +A^\intercal P A$, we have, from Lemma~\ref{lemmaWeyl}, $\lmin{P-A^\intercal P A}+\lmax{A^\intercal P A}\leq \lmax{P}$. Now, $A^\intercal P A\succeq 0$ from $P\succ 0$ and then $\lmax{A^\intercal P A}\geq 0$. We conclude that $\varepsilon\leq 1$. Now, if $\lmin{P-A^\intercal P A}=\lmax{P}$ i.e. $\varepsilon=1$, we have 
%$\lmax{A^\intercal P A}=0$ and then $x^\intercal A^\intercal P A x=0$ for all $x\in \rd$ and $A$ is the null matrix. 
%Finally, $\varepsilon<1$. 
%
%Now from Lemma~\ref{lemma1}, $\varepsilon P=P\lmax{P}^{-1}\lmin{P-A^\intercal P A}\preceq \lmin{P-A^\intercal P A}Id_d\preceq 
%P-A^\intercal P A$ which implies that $(1-\varepsilon) P -A^\intercal P A\succeq 0$. 
%\end{proof}

The result presented in Lemma~\ref{lyapnorm} explains the choice of the matrix norm. It is well-known that $\rho(A)<1$ is equivalent to the existence to a matrix norm $\norm{\cdot}$ for which $\norm{A}<1$. Here we exhibit a compatible norm. We could choose another norm but we would have to restrict the class of systems that we could analyze.    

Now we prove that the argument of the natural logarithm at the numerator appearing in $K(t,P)$ lies in $(0,1]$. 

\begin{lemma}
\label{numerator}
For all $t>0$ and $P\in\lyap{A}$ such that $P\in\lyat{A,Q}$: \[
\left(\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)\right) t^{-1/2}\mu(P)^{-1}\in (0,1]\enspace .
\]
\end{lemma}
\begin{proof}
In appendix.
\end{proof}
%\begin{proof}
%First $t>0$ and $P\succ 0$ then $t^{-1/2}\mu(P)^{-1}>0$ and $\vqQ(t,P)\geq 0$. Using Assumption~\ref{assumR}, we get $(\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)> \vqQ(t,P)-\vqQ(t,P)=0$. We conclude that $\left((\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)\right) t^{-1/2}\mu(P)^{-1}>0$. 
%
%Now, since $\lfrak$ and $\vqQ(t,P)$ are non-negative and for all $a,b\geq 0$, $\sqrt{a+b}\leq \sqrt{a}+\sqrt{b}$  then : 
%$(\lfrak+\vqQ(t,P)^2)^{1/2}\leq \lfrak^{1/2}+\vqQ(t,P)$. Hence : 
%\[
%\begin{array}{lll}
%&\left((\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)\right) t^{-1/2}\mu(P)^{-1}& \\
%\leq &\lfrak^{1/2} t^{-1/2}\mu(P)^{-1} & \\
%\leq &(\displaystyle{\sup_{x\in\xin} x^\intercal Q x})^{1/2} t^{-1/2}\mu(P)^{-1} & \text{ from the def. of } \lfrak
%\end{array}
%\]
%
%Since $t>0$ and $P\in\lyap{A}$ satisfy $P\in\lyat{A,Q}$, then $t P-Q\succeq 0$. It follows that 
%$t x^\intercal P x\geq x^\intercal Q x$ for all $x\in \xin$. Taking the supremum over $\xin$ leads to $(\sup_{x\in\xin} x^\intercal Q x)\mu(P)^{-2}t^{-1} \leq 1$. 
%\end{proof}

\begin{prop}
\label{comp-prop}
The following assertions are true:
\begin{itemize}
\item For all $P\in\lyap{A}$, the function defined on $\cont(P)$, $t\mapsto K(t,P)$ is increasing. Then : 
\begin{equation}
\label{infsolved}
\inf_{P\in\lyap{A}} \inf_{t\in\cont(P)} K(t,P)=\inf_{P\in\lyap{A}}K(\lmax{P^{-1/2} Q P^{-1/2}},P)
\end{equation}
\item For all $t>0$, for all $P\in\lyat{A,Q}$, $K(t,P)=K(1,tP)$ and thus:
\[
\inf_{t>0} \inf_{P\in\lyat{A,Q}} K(t,P)=\inf_{P\in\lyaun} K(1,P)
\]
\end{itemize}
\end{prop}

\begin{proof}
In appendix.
\end{proof}
%\begin{proof}
%Let us prove the first assertion.
%Let $P\in\lyap{A}$. Let $t\in\cont(P)$. The integer part is increasing then we have to prove that the argument of the integer part is increasing in $t$. From Lemma~\ref{lemma2}, we know that $\ln{\norm{A}_P}<0$. Hence, since the natural logarithm is increasing it suffices to show that $\varphi:t:\mapsto \left(\sqr{\lfrak+\vqQ(t,P)^2}-\vqQ(t,P)\right)\left(\sqr{t}\mu(P)\right)^{-1}$ is decreasing. However :
%\[
%\begin{array}{lll}
%\varphi(t)&=&\dfrac{\lfrak}{\left(\sqr{\lfrak+\vqQ(t,P)^2}+\vqQ(t,P)\right)\sqr{t}\mu(P)}\\
%\\
%&=&\dfrac{\lfrak}{\left(\sqr{\lfrak+\dfrac{\norm{q}_2^2 }{4t\lmin{P}}}+\dfrac{\norm{q}_2 }{2\sqr{t\lmin{P}}}\right)\sqr{t}\mu(P)}\\
%\\
%&=&\dfrac{\lfrak}{\left(\sqr{t\lfrak+\dfrac{\norm{q}_2^2 }{4\lmin{P}}}+\dfrac{\norm{q}_2 }{2\sqr{\lmin{P}}}\right)\mu(P)}
%\end{array}
%\]
%We conclude that $t\mapsto \varphi(t)$ is decreasing as an inverse of an increasing function. 
%Finally, Eq.~\eqref{infsolved} follows from Prop.~\ref{propcont} and $t\mapsto K(t,P)$ is increasing for all $P\in\lyap{A}$.  
%
%Now, let $t>0$ and $P\in\lyat{A,Q}$. To prove $K(t,P)=K(1,tP)$ we have to show that $\vqQ(t,P)=\vqQ(1,tP)$, $\sqrt{t}\mu(P)=\mu(tP)$
%and $\norm{A}_{tP}=\norm{A}_P$. The fact $\vqQ(t,P)=\vqQ(1,tP)$ comes from the fact that $\lmin{tP}=t\lmin{P}$ since $t>0$. Second the supremum is homogeneous and then $\sqrt{t}\mu(P)=\mu(tP)$. Finally, $\norm{A}_{tP}=\norm{A}_P$ is a direct consequence of the definition given at Eq.~\eqref{normdef}. We conclude that :
%\[
%\begin{array}{llll}
%\displaystyle{\inf_{t>0} \inf_{P\in\lyat{A,Q}} K(t,P)}&=&\displaystyle{\inf_{t>0} \inf_{P\in\lyat{A,Q}} K(1,tP)}& \text{ from } K(t,P)=K(1,tP)\\
%&=&\displaystyle{\inf_{t>0} \inf_{tP\in\lyaun} K(1,tP)}&\text{ from Eq.~\eqref{simpleequiv}} \\ 
%&=&\displaystyle{\inf_{t>0} \inf_{P\in\lyaun} K(1,P)}&\\
%&=&\displaystyle{\inf_{P\in\lyaun} K(1,P)}&
%\end{array}
%\]
%\end{proof}

%\begin{prop}
%Suppose that $q=0$ and let $P\in\lyaun$. Then: \[K(1,P)=1\iff\sup_{x\in \xin} x^\intercal P x=\sup_{x\in\xin} x^\intercal Q x\enspace.\] 
%\end{prop}

We now present the main result of the paper.
\begin{theorem}
\label{thfond}
Let us define \[\bigk=\min\left\{\inf_{P\in\lyaun} K(1,P),\inf_{P\in\lyap{A}} K(\lmax{P^{-1/2}QP^{-1/2}},P)\right\}\enspace .\] Let $k\geq \bigk$. Then : 
\[
\max_{x\in \xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\leq \lfrak
\]
\end{theorem}
\begin{proof}
In appendix.
\end{proof}
%\begin{proof}
%Let $t>0$ and $P\in\lyap{A}$ such that $P\in\lyat{A,Q}$. Let also $k\in\nn$ and $x\in\xin$. We have:
%\[
%x^\intercal (A^k)^\intercal Q A^k x  
%\leq  t \norm{A^k x}_P^2\leq t\norm{A}^{2k}_P \norm{x}^2_P
%\leq t\norm{A}_P^{2k} \mu(P)^2
%\]
%The first inequality comes from the definition of $\lyat{A,Q}$, the second from the matrix norm definition and the last from the definition of $\mu(P)$ (see Eq.~\eqref{intgfond}).
%
%We also have, for all $x\in\xin$ :
%\[
%q^\intercal A^k x
%\leq \norm{q}_2 \norm{A^k x}_2\leq \dfrac{\norm{q}_2 \norm{A^k x}_P}{\sqr{\lmin{P}}} 
%\leq \dfrac{\norm{q}_2\norm{A}_P^{k} \mu(P)}{\sqr{\lmin{P}}}
%\]
%The first inequality comes from Cauchy-Schwarz, the second from Lemma~\ref{lemma1} and the last from the matrix norm definition and the definition of $\mu(P)$. 
%
%Summing the two parts leads to :
%\[
%x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\leq \left(\sqrt{t}\norm{A}_P^k\mu(P)+\vqQ(t,P)\right)^2-\vqQ(t,P)^2
%\]
%where $\vqQ$ is defined at Eq.~\eqref{intgfond}. Then, $ \left(\sqrt{t}\norm{A}_P^k\mu(P)+\vqQ(t,P)\right)^2-\vqQ(t,P)^2\leq \lfrak$ implies that $ \max_{x\in \xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\leq \lfrak$. Now, we remark that  $\left(\sqrt{t}\norm{A}_P^k\mu(P)+\vqQ(t,P)\right)^2-\vqQ(t,P)^2\leq \lfrak$ is equivalent to :
%\[
%\norm{A}_P^k\leq \left(\sqr{\lfrak+\vqQ(t,P)^2}-\vqQ(t,P)\right)\left(\sqr{t}\mu(P)\right)^{-1}
%\]
%Using the natural logarithm and Lemma~\ref{lyapnorm}, the condition  $k\geq K(t,P)$ is sufficient for the latter inequality .  The latter proof is valid for all couple $t>0$ and $P\in\lyap{A}$ such that $P\in\lyat{A,Q}$. So it remains true for   $\btilde>0$ and $\bP\in\lyap{A}$ such that $P\in\lyat{A,Q}$ and $\bigk=K(\btilde,\bP)$. 
%\end{proof}
We recall that $\lfrak$ is smaller than $\max_{x\in\xin} x^\intercal {A^{\ks}}^\intercal Q A^{\ks} x+q^\intercal A^{\ks} x$. Hence, the optimal value of Problem~\ref{pbopt} is greater than $\lfrak$ since $A^{\ks} (\xin)\subseteq \rea$. We conclude that, following Th.~\ref{thfond}, the optimal value of Problem~\ref{pbopt} is attained for powers of $A$ between $\ks$  and $\max\{\bigk-1,\ks\}$.

\begin{corollary}
The following statement is true :
 \[
\max_{y\in\rea} y^\intercal Q y+q^\intercal y=\max_{\ks\leq k\leq \max\{\bigk-1,\ks\}} \max_{x\in \mathcal{E}(\xin)} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x
\]
\end{corollary}

%\begin{proof}
%Let $k\in\nn$. We define :
%\[
%\nu_k:x\mapsto x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x 
%\]
%Using extreme points instead of the whole set $\xin$ is a direct consequence of the convexity of the functions $\nu_k$ (Assumption~\ref{assum1}) and Lemma~\ref{lemma2}.
%
%Now we write $k^*=\max\{\bigk-1,\ks\}$. We have :
% \[
%\max_{y\in\rea} y^\intercal Q y+q^\intercal y
%=\max_{x\in \mathcal{E}(\xin)}\max\left\{\max_{0\leq k\leq \ks} \nu_k(x), \max_{\ks \leq k\leq k^*} \nu_k(x),\max_{k>k^*} \nu_k(x)\right\}
%\]
%
%%&=&\displaystyle{\max\left\{\max_{0\leq k\leq \bigk-1}\max_{x\in \mathcal{E}(\xin)} \nu_k(x),\lfrak\right\}}
%%\end{array}
%%\]
%Now, for all $k< \ks$, for all $x\in\xin$, $\nu_k(x)\leq 0$ and for all $x\in\xin$, $\nu_{\ks}(x)\leq \lfrak$. Moreover, from Th~\ref{thfond}, 
%for all $k>k^*$, $\nu_k(x)\leq \lfrak$. The conclusion follows from $\displaystyle{\max_{x\in\xin} \max_{\ks\leq k\leq k^*} \nu_k(x)\geq \max_{x\in\xin} \nu_{\ks}(x)\geq \lfrak}$. 
%%The conclusion follows from the fact that  $\lfrak\leq \max_{x\in \mathcal{E}(\xin)} \nu_{\ks}(x)$.
%\end{proof}
The integer $\bigk$ is the best integer than we can get in theory. In practice, we cannot compute $\bigk$. We will solve others semi-definite problems but we will still use the function $(t,P)\mapsto K(t,P)$. We will describe the practical approach at Section~\ref{computations}.

%since we cannot solve the minimization problems :
%\[
%\inf_{P\in\lyaun} K(1,P)\text{ and }\inf_{P\in\lyap{A}}K(\lmax{P^{-1/2} Q P^{-1/2}},P)
%\]
%%In practice, for the moment, we only compute two integers : $K( 1,P)$ where $P$ is an optimal solution of a perturbation of the minimization problem : 
%%\begin{equation}
%%\label{minlmaxQ}
%%\inf\{\lmax{P}\mid P\in \lyaun\}
%%\end{equation}
%%and $K(\lmax{P^{-1/2} Q P^{-1/2}},P)$ where $P$ is an optimal solution of a perturbation of the minimization problem :
%%\begin{equation}
%%\label{minlyap}
%%\inf\{\lmax{P}\mid P\in\lyap{A}\}\enspace .
%%\end{equation}
%Then we take the minimum of the  two integers. We come back latter on the perturbation of the two minimization problems at Section~\ref{computations}.
\subsection{From linear systems to affine ones}
\label{affine}
We come back to Problem~\eqref{context}: the case where the system is affine ($b\neq 0$).
A possible natural reformulation of affine systems into linear one is to use lift-and-project. However, lift-and-project would add 1 as eigenvalue of the matrix defining the new linear system. Assumption~\ref{assum2} would be violated.  Consequently, we adopt another basic approach which consists in using an auxiliary linear discrete-time system.   

 We recall that we have :
%Let us consider the same problem where we replace the linear dynamics by an affine one:
\[
x_0\in \xin,\ \forall\, k\in\nn,\ x_{k+1}=A x_k+b,\
\]
where $A$ is a $d\times d$ matrix and $b$ a vector of $\rd$. We recall that $\rea$ is given by Formula~\eqref{reach} and we consider the optimization problem:
\[
\sup_{x\in\rea} x^\intercal Q x+q^\intercal x
\]
Assumption~\ref{assum2} still holds and it implies that $\Idd-A$ is invertible. It is well-known that the sequence defined by :
\[
\forall\, k\in\nn,\ y_k=x_k-\btilde,\text{ where } \btilde=(\Idd-A)^{-1} b
\]
satisfies:
\[
\forall\, k\in\nn,\ y_{k+1}=A y_k. 
\]
Finally, we have :
\[
\forall\, k\in\nn,\ x_k=A^k y_0+\btilde
\]
This latter expression leads to a new formulation of Problem~\eqref{pbopt}: 
\[
\sup_{k\in\nn} \sup_{y_0\in\xin-\btilde} \left(A^k y_0+\btilde\right)^\intercal Q  \left(A^k y_0+\btilde\right)+q^\intercal  \left(A^k y_0+\btilde\right)
\]
or
\[
\sup_{k\in\nn} \sup_{y_0\in\xin-\btilde} y_0^\intercal (A^k)^\intercal Q  A^k y_0+(2\btilde^\intercal Q+q^\intercal) A^k y_0 +\btilde^\intercal Q \btilde+q^\intercal \btilde
\]
We conclude that we can use the results developed in Subsection~\ref{mainsub} where the matrix $Q$ is unchanged and the vector $q$ to use is now $2Q\btilde+q$. The polytope of initial conditions also changes since we have to consider now $\xin-\btilde$. We have to adapt Assumption~\ref{assumR} with the new linear part equal to $2Q\btilde+q$ and the new initial polytope equal to $\xin-\btilde$. Note that $\mathcal{E}(\xin-\btilde)=\mathcal{E}(\xin)-\btilde$.

\section{Practical computations of $K(t,P)$}
\label{computations}
The computations have been performed using Matlab. We use classical internal routines for powers of matrices (needed for $A^{\ks}$), the vector Euclidean norm (needed for $\norm{q}_2$) and the inverse of $\Idd-A$. We discuss here the computations that rely on semi-definite programming. The solver used in the development is Mosek~\cite{andersen2000mosek} interfaced with Yalmip~\cite{Lofberg2004}. 

To compute $\bigk$, we need to solve two non-convex problems on the cone of positive semi-definite matrices:
\[
\inf\{K(1,P)\mid P\in\lyaun\} \text{ and }\inf\{K(\lmax{P^{-1/2}QP^{-1/2}},P)\mid P\in\lyap{A}\} \enspace.
\]

The non-convexity of the two objective functions do not allow us to use classical solvers to compute these two integers. For the moment, to solve exactly the two problems is left open. Consequently, in practice, we compute an element of $P_1\in\lyaun$ and an element of $P_0\in\lyap{A}$ and take the minimum between $K(1,P_1)$ and $K(\lmax{P_0^{-1/2}QP_0^{-1/2}},P_0)$. The elements $P_0$ and $P_1$  are computed as optimal solutions of linear semi-definite programs. We tried five objective functions :
\begin{equation}
\label{objsdp}
\begin{array}{c}
\displaystyle{F_0(P):=\sup_{x\in\xin} x^\intercal P x}\ \quad \text{ and } \quad\ \displaystyle{F_1(P):=F_0(P-Q)} \\
\\
\displaystyle{F_2(P):=\sum_{x\in\xin} x^\intercal P x}\ \quad \text{ and } \quad\ F_3(P):=F_2(P-Q)\\
\\
F_4(P):=\lmax{P}
\end{array}
\end{equation} 
%First we remark that the computation of $\bigk$ needs to solve two minimization problems $\inf\{K(1,P)\mid P\in\lyaun\}$ and 
%$\inf\{K(\lmax{P^{-1/2}QP^{-1/2}},P)\mid P\in\lyap{A}\}$. To  
%
%To compute an interesting integer $K(t,P)$, we tried three objective functions to compute a matrix $P$. Indeed, we do not compute $t$ and $P$ in the same time. Either, we compute $t$ before $P$ or the converse. If $t$ is computed first, we have $t=1$ following the second assertion of Prop.~\ref{comp-prop} and we compute a $P\in\lyaun$. It $P$ is computed first as an element of $ \lyap{A}$, then $t=\lmax{P^{-1/2}QP^{-1/2}}$ following the first assertion of Prop.~\ref{comp-prop} . These two 
%
%In the computations of the two integers, we need first a matrix that satisfies the discrete Lyapunov equation. This matrix equation 
%appears as a constraint in the semi-definite programs. In this paper, we propose three objective convex functions on the convex cone
%of semi-definite matrix  :
%
%For the computation of $K(1,P)$, we need $P \in \lyaun$. Twhere $\varepsilon$ is set to $0.01$.hus, 
%
%We propose to compute this matrix $P$ as an optimal solution  of the semi-definite problem:

For each objective functions, we add the desired constraints (up to perturbation $\varepsilon=0.01$ to ensure the strict positiveness of discrete Lyapunov equation matrix constraint), we get two families of problems parameterized by $i=0,1,2,3,4$:

\begin{equation}
\label{eq:kunp}
\begin{array}{llc}
\Min & &F_i(P)\\
 & \st &\left\{\begin{array}{l} 
         P-Q\succeq 0\\
         P-A^\intercal P A-\varepsilon \Idd\succeq 0\\
         P\succeq  0
        \end{array}\right.
\end{array}
\end{equation}

and

\begin{equation}
\label{eq:klmaxp}
\begin{array}{llc}
\Min & & F_i(P)\\
 & \st &\left\{\begin{array}{l} 
         P-A^\intercal P A-\varepsilon \Idd\succeq 0\\
         P\succeq  0
        \end{array}\right.
\end{array}
\end{equation}
Problems ~\eqref{eq:kunp} and~\eqref{eq:klmaxp} with the objective functions $F_0$ and $F_1$ can be rewritten as linear semi-definite programs. Indeed they are equivalent to minimize $t$ such that $t\geq x^\intercal P x$ for all $x\in\mathcal{E}( \xin)$ ($P$ being constrained to be semi-definite positive). Note also that the constraint $P-A^\intercal P A-\varepsilon \Idd\succeq 0$ implies that $P$ is actually positive definite.  
We will write $P_1$ for an optimal solution of Pb.~\eqref{eq:kunp} and $\pmax$ for an optimal solution of Pb.~\eqref{eq:klmaxp}. 


Once $\pmax$ computed, we have to compute $\lmax{\pmax^{-1/2}Q \pmax^{-1/2}}$ which can be done by semi-definite programming:
\[
\lmax{\pmax^{-1/2}Q \pmax^{-1/2}}=\inf\{\alpha>0\mid \alpha \pmax-Q\succeq 0\}
\] 

Now, for both $P_1$ and $\pmax$ we have to compute $\lmin{\cdot}$, $\mu(\cdot)$ and $\norm{A}_{\cdot}$. Following Lemma~\ref{lemma1}, since $P_1$ and $\pmax$ are symmetric, the minimal eigenvalue
of those matrices can be computed from semi-definite programming. Indeed for all symmetric matrix $M$, we have : 
\[
\lmin{M}=\sup\{\alpha>0\mid M-\alpha \Idd\geq 0\}
\] 
To compute $\mu(P_1)$ and $\mu(\pmax)$ we suppose that the number of vertices of $\xin$ is small and, following Lemma~\ref{lemma2}, we evaluate directly $\mu(\cdot)$ by enumeration of the image of vertices of $\xin$ and choose the maximal value in the finite list. In a future work, we consider scalable techniques to compute the exact value of $\mu(\cdot)$. The square root (as it is increasing) involved $\mu(\cdot)$ can be performed at the end of the maximization process: we compute the values $x^\intercal P_1 x$ (resp. $x^\intercal \pmax x$) for all vertices; take the maximum of all of them and finally take the square root of the maximal value. 

Finally, following Eq.~\eqref{normdef}, the value $\norm{A}_\cdot$ is compute from semi-definite programming; taking the square root of the optimal value.

To complete the computations, we have to deal with $\lfrak$. We suppose that the integer $\ks$ is given and then to compute $\lfrak$ we employ the same technique used for $\mu(\cdot)$. It suffices to browse the vertices of $\xin$ once since we evaluate, for each vertex, $\min\{x^\intercal Q x, x^\intercal {A^{\ks}}^\intercal Q {A^{\ks}}x+q^\intercal A^{\ks} x\}$ and take the maximal value. 
\section{Experiments}
\label{experiments}
We illustrate our techniques on two academic examples, one linear system and the other affine.
For each example, we describe the method for two property proofs. The first deals with a sublevel of homogeneous quadratic function whereas the second deals with non-homogeneous quadratic function. 
%We decompose the experiments into two sub-classes: the linear and affine systems. Those sub-classes are separeted in three parts : the  case where the objective function is a quadratic homogeneous function; the linear case and the non-homogeneous quadratic objective function. For each cut, we provide a numerical example.  
\subsection{Linear systems}

\subsubsection{Homogeneous objective function}
\label{exp-linear}
We consider the discretisation of an harmonic oscillator $\ddot{x}+\dot{x}+x=0$ by an explicit Euler scheme. The discretization step $h$ is set to 0.01. Introducing the position variable, $x$ and the speed variable $v$. We assume that the initial conditions can be taken into the set $[-1,1]^2$. The Euler scheme becomes a linear discrete-time system in dimension two defined as follows: 
\begin{equation}
\label{harmonic}
\begin{pmatrix}
x_{k+1}\\
v_{k+1}
\end{pmatrix}
=\begin{pmatrix}
1 & h\\
-h & 1-h
\end{pmatrix}
\begin{pmatrix}
x_{k}\\
v_{k}
\end{pmatrix},\ (x_0,v_0)\in [-1,1]^2
\end{equation}
We want to prove that :
the trajectories are bounded and compute a bound over the Euclidean norm of the state-variable; for all $k\in\nn$, $x_{k}^2$ and $v_{k}^2$ are less than 1.
Note that Assumption~\ref{assumR} holds as the hypothesis of Prop~\ref{interiorprop} is satisfied.
\paragraph{Boundedness}. The boundedness is already proved since $\rho(A)<1$ and $A^k x$ tends to 0 as $k$ tends to $+\infty$. However, we do not have a bound over the values taken by the state-variable. To bound these values, we compute the maximum of the Euclidean norm of the state-variable.  

\vspace{-0.4cm}

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{6}{|c|}{$\norm{(x_k,v_k)}_2^2$}\\
\hline
Numbers/Objective functions & $F_0$ & $F_{1}$ & $F_{2}$ & $F_3$ & $F_4$ \\
\hline
$K_{\rm max}$ &135 &135 &130 &130 &130 \\
\hline
 $K_{1}$ & 169& 169&169 &169 & 169\\
\hline
\end{tabular}
\end{center}
\caption{The table of integers computed for Linear System~\eqref{harmonic} from objective functions $F_i$ defined at Eq~\eqref{objsdp} to prove the boundedness.}
\end{table}

\vspace{-0.4cm}

Actually, the maximum of the square of the Euclidean norm is reached at $k=0$ (for initial conditions) and is equal to $2$. Our computations show that we have to stop at $k=130$. Note that the sequence $\sup_{x\in\xin} \norm{A^k x}_2^2$ is not decreasing i.e. $\Idd$ is not a solution of the discrete Lyapunov matrix equation. 


 \paragraph{The proof of $x_{k}^2\leq 1$}. 
 The maximum of the square of the first coordinate is reached at $k=61$ and is equal to $1.6489$. Our computations show that we have to stop at $k=188$. This disproves the property i.e. for all $k\geq 0$, $x_k^2\leq 1$ does not hold.
 
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{6}{|c|}{$x_k^2$}\\
\hline
Numbers/Objective functions & $F_0$ & $F_{1}$ & $F_{2}$ & $F_3$ & $F_4$ \\
\hline
$K_{\rm max}$ &230 &230 & 230&230 &230 \\
\hline
 $K_{1}$ &188 &188 &188 &188 &188 \\
\hline
\end{tabular}
\end{center}
\caption{The table of integers computed for Linear System~\eqref{harmonic} from objective functions $F_i$ defined at Eq~\eqref{objsdp} to prove $x_k^2\leq 1$.}
\end{table}

\vspace{-0.2cm}

\paragraph{The proof of $v_{k}^2\leq 1$}.
The maximum of the square of the second coordinate is reached at $k=0$ (for initial conditions) and is equal to $1$. Our computations show that we have to stop at $k=221$. Consequently, the property holds. Note that the sequence $\sup_{x\in\xin} (A^k x)_2^2$ is not decreasing.

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{6}{|c|}{$v_k^2$}\\
\hline
Numbers/Objective functions & $F_0$ & $F_{1}$ & $F_{2}$ & $F_3$ & $F_4$ \\
\hline
$K_{\rm max}$ &233 &233 &221 & 221&227 \\
\hline
 $K_{1}$ &261 &261 &261 &261 &261 \\
\hline
\end{tabular}
\end{center}
\caption{The table of integers computed for Linear System~\eqref{harmonic} from objective functions $F_i$ defined at Eq~\eqref{objsdp} to prove $v_k^2\leq 1$.}
\end{table}

\subsubsection{Non-Homogeneous objective function}
Let us consider the same linear system depicted at Eq~\ref{harmonic}. We are interested in proving that for all $k\in\nn$, $-2\leq x_k-0.5v_k\leq 3$. Following Remark~\ref{remarklinear}, this is the same as proving $(x_k-0.5 v_k)^2-x_k+0.5 v_k\leq 6$. Then, we have to compute the optimal value :
\[
\sup_{k\in\nn} x_k^\intercal Q x_k+q^\intercal x_k
\]
where $Q=\begin{pmatrix} 1 & -1/2 \\ -1/2 & 1/4\end{pmatrix}$ and $q^\intercal=(-1,1/2)$. 

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{6}{|c|}{$(x_k-0.5 v_k)^2-x_k+0.5 v_k$}\\
\hline
Numbers/Objective functions & $F_0$ & $F_{1}$ & $F_{2}$ & $F_3$ & $F_4$ \\
\hline
$K_{\rm max}$ & 285&285 &285 &285 &285 \\
\hline
 $K_{1}$ &276 &276 &279 &279 &262 \\
\hline
\end{tabular}
\end{center}
\caption{The table of integers computed for Linear System~\eqref{harmonic} from objective functions $F_i$ defined at Eq~\eqref{objsdp} to prove $-2\leq x_k-0.5v_k\leq 3$.}
\end{table}

The table shows that we should stop at the iteration $k=262$ to ensure that the maximum of $(x_k-0.5 v_k)^2-x_k+0.5 v_k$ is reached before the step. Actually the maximum is reached at iteration $k=0$ and is equal to $3.75$ which indicates that the property does hold since we wanted to prove that
$(x_k-0.5 v_k)^2-x_k+0.5 v_k\leq 6$. 
\subsection{Affine systems}

\subsubsection{Homogeneous objective function}

%Rotation plus translation 
We propose to use the same linear system proposed by Ahmadi and G\"unl\"uk in~\cite{ahmadi2018robust} governed by a rotation transformation except that we add a translation in the geometric transformation. For record, their system is governed by the (quasi) rotation matrix:

\[
A=
 \dfrac{4}{5}\begin{pmatrix}
 \cos(\theta)& sin(\theta)\\
-sin(\theta) & \cos(\theta)
\end{pmatrix},\text{ where }\theta=\dfrac{\pi}{6}
\]

Note that since $(5/4)A$ is a rotation matrix, then for all $x\in\rr^2$,
$(25/16)\norm{Ax}_2^2=\norm{x}_2^2$ and thus $\Idd-A^\intercal \Idd A\succeq 0$ and $\Idd$ is a solution of the discrete Lyapunov equation. From Prop~\ref{lyapunovbigk}, we have $\bigk=K(1,\Idd)=1$. 

In practice, our Matlab implementation found $Q$ as optimal solution for Problem~\eqref{eq:kunp} when the objective functions are $F_i$ with $i=1,2,3,4$ (all of them except $F_0(P)=\sup_{x\in \xin} x^\intercal P x$). For the case of optimal solutions of Problem~\eqref{eq:klmaxp}, $Q$ is never returned. In the two cases, $K(t,P)=1$ whatever the objective functions.
  
In our experiment, we add a translation tranformation characterized by the vector $(1,-1)^\intercal$. We get the system:
\begin{equation}
\label{affinesys}
\begin{pmatrix}
x_{k+1}\\
y_{k+1}
\end{pmatrix}
= \dfrac{4}{5}\begin{pmatrix}
 \cos(\theta)& sin(\theta)\\
-sin(\theta) & \cos(\theta)
\end{pmatrix}
\begin{pmatrix}
x_{k}\\
y_{k}
\end{pmatrix}+\begin{pmatrix}
1\\
-1
\end{pmatrix},\ (x_0,y_0)\in [-1,2]^2
\end{equation}

We want to prove that for all $k\in\nn$, $x_k^2\leq 16$ and $y_k^2\leq 16$.

Note that Assumption~\ref{assumR} holds as the hypothesis of Prop~\ref{interiorprop} is satisfied.
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{6}{|c|}{$x^2_k$}\\
\hline
Numbers/Objective functions & $F_0$ & $F_{1}$ & $F_{2}$ & $F_3$ & $F_4$ \\
\hline
$K_{\rm max}$ & 6&6 & 6&9&6 \\
\hline
 $K_{1}$ & 97& 97& 94&94 &8 \\
\hline
\end{tabular}
\end{center}
\caption{The table of integers computed for Affine System~\eqref{affinesys} from objective functions $F_i$ defined at Eq~\eqref{objsdp} to prove $x_k^2\leq 16$.}
\end{table}
Actually, the maximum of the square of the first coordinate is reached at $k=1$ and is equal to $10.1483$. Our computations show that we have to stop at $k=6$. This proves the property i.e. for all $k\geq 0$, $x_k^2\leq 16$ does hold. 

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{6}{|c|}{$y_k^2$}\\
\hline
Numbers/Objective functions & $F_0$ & $F_{1}$ & $F_{2}$ & $F_3$ & $F_4$ \\
\hline
$K_{\rm max}$ &11 &11 &11 &12 &11 \\
\hline
 $K_{1}$ &264 & 273&263 & 263&19 \\
\hline
\end{tabular}
\end{center}
\caption{The table of integers computed for Affine System~\eqref{affinesys} from objective functions $F_i$ defined at Eq~\eqref{objsdp} to prove $y_k^2\leq 16$.}
\end{table}

Actually, the maximum of the square of the first coordinate is reached at $k=4$ and is equal to $21.1427$. Our computations show that we have to stop at $k=11$. This disproves the property i.e. for all $k\geq 0$, $y_k^2\leq 16$ does not hold. 


\subsubsection{Non-Homogeneous objective function}

Taking the same system depicted at Eq~\eqref{affinesys}, we want to prove that $-7\leq 0.5x_k-2y_k\leq 5$ is invariant by the dynamics. From Remark~\ref{remarklinear}, to prove the property is equivalent to prove $(0.5x_k-2 y_k)^2-x_k+4y_k\leq 35$. Then, we have to compute the optimal value :
\[
\sup_{k\in\nn} x_k^\intercal Q x_k+q^\intercal x_k  
\]
where $Q=\begin{pmatrix} 1/4 & -1 \\ -1 & 4\end{pmatrix}$ and $q^\intercal=(-1,4)$.  
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{6}{|c|}{$(x_k-0.5 v_k)^2-x_k+0.5 v_k$}\\
\hline
Numbers/Objective functions & $F_0$ & $F_{1}$ & $F_{2}$ & $F_3$ & $F_4$ \\
\hline
$K_{\rm max}$ & 11&11 &11 &12 &11 \\
\hline
 $K_{1}$ & 27 &1100 &1114 &1114 &18 \\
\hline
\end{tabular}
\end{center}
\caption{The table of integers computed for Affine System~\eqref{affinesys} from objective functions $F_i$ defined at Eq~\eqref{objsdp} to prove $-7\leq 0.5x_k-2y_k\leq 5$.}
\end{table}

The table shows that we should stop at the iteration $k=11$ to ensure that the maximum of $(0.5x_k-2 y_k)^2-x_k+4y_k$ is reached before the step $k=11$. Actually the maximum is reached at iteration $k=4$ and is equal to $73.295$ which indicates that the property does hold since we wanted to prove that
$(x_k-0.5 v_k)^2-x_k+0.5 v_k\leq 35$.

\section{Conclusion and Future Works}
\label{conclusion}
In this paper, we introduce a new class of optimization problems. This class solves verification problem coming from computer science. This class can be described as follows : the objective function is convex and quadratic and the decision variable is constrained to belong to the reachable values set of a discrete-time affine system which is not a convex set. The main issue is the infinite horizon problem. To maximize the function we can wait for an arbitrary long time. Thus the paper proposes to compute, in the case of stable affine system, a finite horizon after which the search of an optimal value is useless. The problem becomes a finite horizon problem which can be solved in a finite time. 

Nevertheless some computational questions are left open. In this paper, we describe the technique on small examples and for which the number of vertices of the initial polyhedron is small. A more scalable approach can be done in a future work using known algorithms to solve the maximization of convex quadratic functions over linear inequalities. 

The most  difficult part resides in the minimization of the functions $P\mapsto K(1,P)$ and $P\mapsto K(\lmax{P^{-1/2} Q P^{-1/2}},P)$ over respectively $\lyaun$ and $\lyap{A}$. The non-convexity of the function makes difficult the minimization procedure.         
\bibliographystyle{alpha}
\bibliography{paperbib}

\newpage

\section*{Appendix}

We recall this useful lemma.

\begin{lemma}
\label{lemma1}
Let $M$ be a symmetric matrix of size $d$. 
%We recall that $\lmax{M}$ (resp. $\lmin{M}$) denotes the greatest eigenvalue of $M$ (resp. the smallest eigenvalue of $M$) 
Then, for all $x\in \rd$:
\begin{equation}
\label{eigineg}
\lmin{M}\norm{x}_2^2\leq x^\intercal M x\leq \lmax{M} \norm{x}_2^2
\end{equation}
where $\norm{\cdot}_2$ is the Euclidean norm. 
\end{lemma}
Inequalities~\eqref{eigineg} can be formulated in term of positive semi-definiteness:
\[
\lmin{M}\Idd\preceq M\preceq \lmax{M}\Idd
\]

%We recall Weyl's inequalities
%~\cite{horn1990matrix} 
We recall Weyl's inequalities that provide inequalities for the eigenvalues of the sum of two symmetric matrices and the sum of the eigenvalues of each matrix.  
\begin{lemma}[Weyl's inequalities]
\label{lemmaWeyl}
Let $M$ and $N$ be symmetric matrices. We have, for all $\ell\in\{1,\ldots,d\}$ :
\[
\lambda_\ell(M)+\lmin{N}\leq \lambda_\ell(M+N)\leq \lambda_\ell(M)+\lmax{N}
\]
\end{lemma}

Here is the proof of Proposition~\ref{propcont}. 

\begin{proof}[Proposition~\ref{propcont}]
Let $P\succ 0$, let $t\in\cont(P)$. Let $x\in\rd$, $x\neq 0$. We have 
$x^\intercal Q x\leq t x^\intercal P x$ and thus $(x^\intercal Q x)(x^\intercal P x)^{-1}\leq t$. Then 
$\sup_{x\neq 0}(x^\intercal Q x)(x^\intercal P x)^{-1}\leq t $. Writing $y=P^{1/2}x$ ( recall that $P^{1/2}$ is invertible as $P\succ 0$), we obtain :
$\sup_{y\neq 0}(y^\intercal P^{-1/2}Q P^{-1/2}y)(y^\intercal y)^{-1}\leq t $. However, we have
$\sup_{z\neq 0} (z^\intercal M z)(z^\intercal z)^{-1}=\lmax{M}$. In fact, $\lmax{M}$ is achieved for an eigenvector associated with $\lmax{M}$. Hence, 
$\sup_{y\neq 0} (y^\intercal P^{-1/2}Q P^{-1/2}y)(y^\intercal y)^{-1}=\lmax{P^{-1/2} Q P^{-1/2}}$. We conclude that $\lmax{P^{-1/2} Q P^{-1/2}}P-Q\succeq 0$ is a direct consequence of Lemma~\ref{lemma1}. 
\qed
\end{proof}

Here is the proof of Lemma~\ref{lyapnorm}.

\begin{proof}[Lemma~\ref{lyapnorm}]
Let $P$ such that $P\succ 0$ and $P-A^\intercal P A\succ 0$.  First, the matrix $A$ is not null then its spectral radius is strictly positive. Then since $\norm{\cdot}_P$ is a matrix norm, $0<\rho(A) \leq \norm{A}_P$.

Secondly, since $P-A^\intercal P A\succ 0$ and using Eq.~\eqref{normdef}, we have $\norm{A}_P\leq 1$. Still using Eq.~\eqref{normdef}, to prove $\norm{A}_P<1$, it suffices to exhibit $\varepsilon>0$ such that $(1-\varepsilon) P-A^\intercal P A\succeq 0$ with $0<\varepsilon<1$. 

Let us write $\varepsilon=\lmin{P-A^\intercal P A} \lmax{P}^{-1}$. Let us prove that $0<\varepsilon<1$ and $(1-\varepsilon) P-A^\intercal P A\succeq 0$.  Since $P-A^\intercal P A\succ 0$, $\lmin{P-A^\intercal P A}>0$ and from $P\succ 0$ we conclude that $\varepsilon >0$. Since $P=P-A^ \intercal P A +A^\intercal P A$, we have, from Lemma~\ref{lemmaWeyl}, $\lmin{P-A^\intercal P A}+\lmax{A^\intercal P A}\leq \lmax{P}$. Now, $A^\intercal P A\succeq 0$ from $P\succ 0$ and then $\lmax{A^\intercal P A}\geq 0$. We conclude that $\varepsilon\leq 1$. Now, if $\lmin{P-A^\intercal P A}=\lmax{P}$ i.e. $\varepsilon=1$, we have 
$\lmax{A^\intercal P A}=0$ and then $x^\intercal A^\intercal P A x=0$ for all $x\in \rd$ and $A$ is the null matrix. 
Finally, $\varepsilon<1$. 

Now from Lemma~\ref{lemma1}, $\varepsilon P=P\lmax{P}^{-1}\lmin{P-A^\intercal P A}\preceq \lmin{P-A^\intercal P A}Id_d\preceq 
P-A^\intercal P A$ which implies that $(1-\varepsilon) P -A^\intercal P A\succeq 0$. \qed
\end{proof}

Here is the proof of Lemma~\ref{numerator}.

\begin{proof}[Lemma~\ref{numerator}]
First $t>0$ and $P\succ 0$ then $t^{-1/2}\mu(P)^{-1}>0$ and $\vqQ(t,P)\geq 0$. Using Assumption~\ref{assumR}, we get $(\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)> \vqQ(t,P)-\vqQ(t,P)=0$. We conclude that $\left((\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)\right) t^{-1/2}\mu(P)^{-1}>0$. 

Now, since $\lfrak$ and $\vqQ(t,P)$ are non-negative and for all $a,b\geq 0$, $\sqrt{a+b}\leq \sqrt{a}+\sqrt{b}$  then : 
$(\lfrak+\vqQ(t,P)^2)^{1/2}\leq \lfrak^{1/2}+\vqQ(t,P)$. Hence : 
\[
\begin{array}{lll}
&\left((\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)\right) t^{-1/2}\mu(P)^{-1}& \\
\leq &\lfrak^{1/2} t^{-1/2}\mu(P)^{-1} & \\
\leq &(\displaystyle{\sup_{x\in\xin} x^\intercal Q x})^{1/2} t^{-1/2}\mu(P)^{-1} & \text{ from the def. of } \lfrak
\end{array}
\]

Since $t>0$ and $P\in\lyap{A}$ satisfy $P\in\lyat{A,Q}$, then $t P-Q\succeq 0$. It follows that 
$t x^\intercal P x\geq x^\intercal Q x$ for all $x\in \xin$. Taking the supremum over $\xin$ leads to $(\sup_{x\in\xin} x^\intercal Q x)\mu(P)^{-2}t^{-1} \leq 1$. 
\qed
\end{proof}

Here is the proof of Proposition~\ref{comp-prop}.

\begin{proof}[Proposition~\ref{comp-prop}]
Let us prove the first assertion.
Let $P\in\lyap{A}$. Let $t\in\cont(P)$. The integer part is increasing then we have to prove that the argument of the integer part is increasing in $t$. From Lemma~\ref{lemma2}, we know that $\ln{\norm{A}_P}<0$. Hence, since the natural logarithm is increasing it suffices to show that $\varphi:t:\mapsto \left(\sqr{\lfrak+\vqQ(t,P)^2}-\vqQ(t,P)\right)\left(\sqr{t}\mu(P)\right)^{-1}$ is decreasing. However :
\[
\begin{array}{lll}
\varphi(t)&=&\dfrac{\lfrak}{\left(\sqr{\lfrak+\vqQ(t,P)^2}+\vqQ(t,P)\right)\sqr{t}\mu(P)}\\
\\
&=&\dfrac{\lfrak}{\left(\sqr{\lfrak+\dfrac{\norm{q}_2^2 }{4t\lmin{P}}}+\dfrac{\norm{q}_2 }{2\sqr{t\lmin{P}}}\right)\sqr{t}\mu(P)}\\
\\
&=&\dfrac{\lfrak}{\left(\sqr{t\lfrak+\dfrac{\norm{q}_2^2 }{4\lmin{P}}}+\dfrac{\norm{q}_2 }{2\sqr{\lmin{P}}}\right)\mu(P)}
\end{array}
\]
We conclude that $t\mapsto \varphi(t)$ is decreasing as an inverse of an increasing function. 
Finally, Eq.~\eqref{infsolved} follows from Prop.~\ref{propcont} and $t\mapsto K(t,P)$ is increasing for all $P\in\lyap{A}$.  

Now, let $t>0$ and $P\in\lyat{A,Q}$. To prove $K(t,P)=K(1,tP)$ we have to show that $\vqQ(t,P)=\vqQ(1,tP)$, $\sqrt{t}\mu(P)=\mu(tP)$
and $\norm{A}_{tP}=\norm{A}_P$. The fact $\vqQ(t,P)=\vqQ(1,tP)$ comes from the fact that $\lmin{tP}=t\lmin{P}$ since $t>0$. Second the supremum is homogeneous and then $\sqrt{t}\mu(P)=\mu(tP)$. Finally, $\norm{A}_{tP}=\norm{A}_P$ is a direct consequence of the definition given at Eq.~\eqref{normdef}. We conclude that :
\[
\begin{array}{llll}
\displaystyle{\inf_{t>0} \inf_{P\in\lyat{A,Q}} K(t,P)}&=&\displaystyle{\inf_{t>0} \inf_{P\in\lyat{A,Q}} K(1,tP)}& \text{ from } K(t,P)=K(1,tP)\\
&=&\displaystyle{\inf_{t>0} \inf_{tP\in\lyaun} K(1,tP)}&\text{ from Eq.~\eqref{simpleequiv}} \\ 
&=&\displaystyle{\inf_{t>0} \inf_{P\in\lyaun} K(1,P)}&\\
&=&\displaystyle{\inf_{P\in\lyaun} K(1,P)}&
\end{array}
\]
\qed
\end{proof}

Here is the proof of Theorem~\ref{thfond}.

\begin{proof}[Theorem~\ref{thfond}]
Let $t>0$ and $P\in\lyap{A}$ such that $P\in\lyat{A,Q}$. Let also $k\in\nn$ and $x\in\xin$. We have:
\[
x^\intercal (A^k)^\intercal Q A^k x  
\leq  t \norm{A^k x}_P^2\leq t\norm{A}^{2k}_P \norm{x}^2_P
\leq t\norm{A}_P^{2k} \mu(P)^2
\]
The first inequality comes from the definition of $\lyat{A,Q}$, the second from the matrix norm definition and the last from the definition of $\mu(P)$ (see Eq.~\eqref{intgfond}).

We also have, for all $x\in\xin$ :
\[
q^\intercal A^k x
\leq \norm{q}_2 \norm{A^k x}_2\leq \dfrac{\norm{q}_2 \norm{A^k x}_P}{\sqr{\lmin{P}}} 
\leq \dfrac{\norm{q}_2\norm{A}_P^{k} \mu(P)}{\sqr{\lmin{P}}}
\]
The first inequality comes from Cauchy-Schwarz, the second from Lemma~\ref{lemma1} and the last from the matrix norm definition and the definition of $\mu(P)$. 

Summing the two parts leads to :
\[
x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\leq \left(\sqrt{t}\norm{A}_P^k\mu(P)+\vqQ(t,P)\right)^2-\vqQ(t,P)^2
\]
where $\vqQ$ is defined at Eq.~\eqref{intgfond}. Then, $ \left(\sqrt{t}\norm{A}_P^k\mu(P)+\vqQ(t,P)\right)^2-\vqQ(t,P)^2\leq \lfrak$ implies that $ \max_{x\in \xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\leq \lfrak$. Now, we remark that  $\left(\sqrt{t}\norm{A}_P^k\mu(P)+\vqQ(t,P)\right)^2-\vqQ(t,P)^2\leq \lfrak$ is equivalent to :
\[
\norm{A}_P^k\leq \left(\sqr{\lfrak+\vqQ(t,P)^2}-\vqQ(t,P)\right)\left(\sqr{t}\mu(P)\right)^{-1}
\]
Using the natural logarithm and Lemma~\ref{lyapnorm}, the condition  $k\geq K(t,P)$ is sufficient for the latter inequality .  The latter proof is valid for all couple $t>0$ and $P\in\lyap{A}$ such that $P\in\lyat{A,Q}$. So it remains true for   $\btilde>0$ and $\bP\in\lyap{A}$ such that $P\in\lyat{A,Q}$ and $\bigk=K(\btilde,\bP)$. 
\qed
\end{proof}
\end{document}
