 \documentclass[10pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{version}
\usepackage{url}

\allowdisplaybreaks
 \newcommand\ForAuthors[1]%          %  temporary remark for the
 {\par\smallskip                     %  authors:
  \begin{center}%                    %
   \fbox%                            %    --------
   {\parbox{0.9\linewidth}%          %    |  #1  |
    {\raggedright\sc--- #1}%         %    --------
   }%                                %
  \end{center}%                      %
  \par\smallskip                     %
 }

\input{raccourcis}

\title{Optimal analysis of Discrete-time Time-Invariant Affine Systems}
\author{Assalé Adjé\\
Université of Perpignan Via Domitia\\
LAMPS\\
France\\
\url{assale.adje@univ-perp.fr}
}
\date{}%

\begin{document}
\maketitle
\begin{abstract}
 Our very first concern is the resolution of the verification problem for the class of discrete-time affine dynamical systems. This verification problem is turned into an optimization problem where the constraint set is the reachable values set of the dynamical system. To solve this optimization problem, we truncate the infinite sequences belonging to reachable values set at some step which is uniform with respect to the initial conditions. In theory, the best possible uniform step is the optimal solution of a non-convex semi-definite program. In practice,  we propose a methodology to compute an uniform step that over-approximate the best solution.
\end{abstract}

\section{Introduction}
\paragraph{Motivations.}
Some catastrophic events~\cite{johnson2005natural,mcquaid2012software} caused by bugs in programs explain the importance of the formal verification of programs in general and static analysis of programs~\cite{cousot2010gentle} in particular. The interest is shared by both theorists and industrials. Indeed, since the success of Astrée~\cite{delmas2007astree,souyris2007experimental,bouissou2009space}, industrials are  more and more concerned with static analysis.  To solve static analysis problems is thus an interesting scientific challenge for the theory as well as for the applications and the pratical  side.   

This interest for static analysis techniques comes from the cover of them. Indeed static analysis offers a deeper analysis compared to traditional test techniques which cannot cover all possible situations. The principle of static analysis consists in proving properties automatically on programs without executing them. Static analyzers only use the structures of programs not the values generated by the program.  Numerous various techniques exist but none of them cannot be complete. This means that no program can validate or disprove a given property for all programs. Static analysis is not only useful for programs but can also be applied to validate properties on dynamical systems in general. The principle is still the same : to prove properties on dynamical systems without any simulations.

To prove or disprove property even for discrete-time linear system is still an open challenge~\cite{acceleration-popl14,7403149,DBLP:conf/icalp/AlmagorCO018}.  The difficulty of the analysis lies in the infinite number of points in the reachable values set. Static analysis abstract interpretation~\cite{cousot2001abstract} based proposes to prove properties by means of over-approximations of reachable values set. Then abstract interpretation has difficulties to disprove a property since over-approximations can contain non-reachable values which violate the property.
%Static analysis is mainly divided into two approaches the model-checking and abstract interpretation. Model-checking exploits finite models to disprove property by the construction of a counterexemple whereas abstract interpretation proposes to prove a property by means of over-approximation of reachable values set. Those techniques are then limited since model-checking has difficulties to prove properties and abstract interpretation has difficulties to disprove a property. 

The main motivation of the paper is to propose a technique that can be, for a given discrete-time affine system and a given property, to prove or disprove the property on the system. 
%This problem is widely studied in computer science community~\cite{mine} or in control theory~\cite{safety}. Optimization theory 
%~\cite{optim} are more and more used to solve verification problems. The proposed approach follows this line of research : our verification problem is reduced to solve an optimization problem with dynamical system constraints. Actually this particular subclass of optimization problems is called the robust-to-dynamics optimization~\cite{ahmadi}. 

%To do this, we reformulate the verification problem as an optimization problem with dynamical system contraints. This optimization problem is then solved exactly. The exact optimal solution can guarantee whether the property holds or not. Note that this particular subclass of optimization problem is called robust-to-dynamics optimization in~\cite{7403149}.   

\paragraph{Related works .}
%For the static analysis of discrete-time affine systems, numerous works in computer science or control theory exist. 
%in computer science as we said earlier, works are decomposed into two world model checking community and abstract interpretation. 
%
%Model-checking approach as said earlier can disprove a property by the synthesis of counter-examples. The direct validation of the property would require an infinite number of runs.  
The paper solves a formal verification problem. Numerous works (for example~\cite{DBLP:journals/corr/abs-1111-5223,gonnord2014abstract,mine-ESOP16,Sankaranarayanan_BenSassi__2017__Template}) about the resolution of the problem by abstract interpretation are available. They are mainly concerned in the representation of the reachable values. The over-approximations produced are not sufficient to solve a specific verification problem with a given property.

Hence, we only consider related works which produce a proof for a given property. Moreover, we are focused on close techniques based on optimization theory and or Lyapunov functions to solve the verification problem. 

Proof techniques based on Lyapunov functions~\cite{roozbehani2013optimization,blanchini2009lyapunov} automatically deals with non-convexities. This makes difficult the exact resolution of the verification problem. To use Lyapunov functions to solve verification problems lead  to over-approximations and only conclusion that can be made is "the property holds or we do not know".

In~\cite{adje2015property}, the authors propose  an approach based on Sums-of-Squares optimization to prove property on discrete-time polynomial systems. The verification problem is reduced to an optimization problem exactly the same that we use here.  The objective function is provided from the property to prove. The proof of the property pass through the synthesis of a polynomial invariant i.e.  a sublevel set of a polynomial that over-approximates the reachable value set of the polynomial system. The computed sublevel is the one that minimizes the over-approximation. This approach uses abstract interpretation and then cannot disprove a property. Actually, the main issue is that we cannot prove that there is a "zero duality gap result" . The same kind of approach exists for piecewise affine systems~\cite{10.1007/978-3-319-54292-8_2} with the same disadvantage. 


The closest work seems to be the work of Ahmadi et al~\cite{7403149,ahmadi2018robust} which introduces the class of robust-to-dynamics optimization problem. Ahmadi et al solve the problem where the dynamics and the objective function are linear. Moreover, they need a bounded invariant set to solve their problem. In our work, we deal with non-linear quadratic  forms and affine dynamical systems (to pass from linear to affine seems to be a very small improvment).  Moreover, we do not need a bounded invariant to solve our robust-to-dynamics optimization problem.  Nevertheless, some similarities exist between their approach and the one proposed here : the use of a solution of the discrete Lyapunov matrix equation and the computation of a the number of steps to convergence.  

%In this paper, we are interested in proving properties on discrete-time affine systems. The proof is based on a static analysis approach. It consists in verifying the dynamical system without using simulations and prove the property from the dynamics and the initial conditions.  We are focused on numerical properties. Properties considered can be the boundedness of some state-variables, to belong to some safe set or to avoid an unsafe set.  

%Some works propose numerical optimization techniques to solve verification problem.
%Numerical programs in an ideal world can be viewed as a discrete-time dynamical systems. Some numerical
%programs comes from discretization or linearization techniques. Thus linear systems forms a huge class of interesting problems. 


%During the last twenty years,  the verification of linear systems techniques evolve and propose new challenges. Classical approaches come from theoretical computer science. Those approaches are based on static analysis such as model-checking or abstract interpretation. 
%The goal of the paper is to offer a complete analysis of discrete-time linear systems.
%Indeed, the static analysis of such systems based on abstract interpretation can prove 
%valid properties. However, because abstractions and the presence of false alarms, the challenge which consists in invalidating properties remains extremely difficult for analyzers. The problem addressed here can be reduced to maximize a function over the reachable values set of a dynamical system.
%The method developed here consists in replacing an infinite numbers of maximization problems by a finite number of them.  We propose to compute an integer after which we know that the optimal value cannot be reached. 

\paragraph{Contributions}
Our very first concern is the resolution of the verification problem for the class of discrete-time affine dynamical systems. We assume that the given property is written as the sublevel set of some quadratic function. 
We reduced the verification problem to an optimization problem of the form :
\begin{equation}
\label{eqintro}
\sup\{f(x)\mid x_{k+1}=g(x_k),\ x_0\in\xin\}
\end{equation}
where $f$ is a quadratic function, $g$ an affine function and $\xin$ is a polytope.
Note that this optimization formulation has been already used in previous papers of the author for verification problems. 

The optimal value~\eqref{eqintro} only depends on the initial values i.e. is equal to :
\[
\sup\{f( g^k (x))\mid  x_0\in\xin,\ k\in\nn\}\enspace.
\]
In the paper, we compute a number of steps $\bigk$ after which the search of the optimal value is useless i.e.
 \begin{equation}
\label{eqintro2}
\sup\{f( g^k (x))\mid  x_0\in\xin,\ k\in\nn\}=
\sup\{f( g^k (x))\mid  x_0\in\xin,\ k\in \{0,\ldots,\bigk\}\}
\end{equation}
This number of steps $\bigk$ is valid for all possible initial values. We are looking for the best possible integer $\bigk$ for which 
Equation~\eqref{eqintro2} is true. We propose to construct a family af integers parameterized by some positive reals and matrices solution of a discrete Lyapunov equation. The approach proposed in the paper to construct the family of integeres uses basic tools from matrix theory such as the discrete Lyapunov matrix equation, minimal and maximal eigenvalues, matrix norms and generalized Raleigh quotients. In theory, the best integer that we can propose is then the optimal value of some non-convex semi-definite program.  
The exact resolution of this semi-definite problem is, for the moment, left open and we just provide an over-approximation of its optimal value. By doing so, it does not degrade the exact resolution of the verification problem.   However, for the moment, we propose in practice the computation of two integeres which over-approximate the best value. 
%In practice, we use semi-definite programming to solve the discrete Lyapunov matrix equation and to compute extremal eigenvalues that we need. We end the talk by some experimental results that measure the accuracy of our approach.
%
%Verification linear /affine systems
%Optimization tools for verification
%Formal methods tools 
%Control problems with reachability

\paragraph{Organization of the paper}
Section~\ref{statement} recalls in details the problem statement that is the formulation of our verification problem and its translation into a verification problem. Section~\ref{mainresults} presents the main results. We give details about the construction of the family of integers solutions of Eq.~\eqref{eqintro2}. Section~\ref{compuations} describes the practical idea used in the implementation.  Section~\ref{experiments} is devoted to the experimentation. Finally, Section~\ref{conclusion} concludes and opens future researcjh directions.
\paragraph{Notations}
In this paper, we denote by $\Idd$ the identity matrix of size $d\times d$. We denote, for a matrix $M$, by $M^\intercal$ the transpose of $M$. The set of the (square) symmetric matrices  (the matrices $M$ such that $M=M^\intercal$) of size $d$ is denoted by $\sym$.  

We recall that a matrix $M$ of size $d\times d$ is {\it positive semidefinite} if and only if $M$ is symmetric for all $x\in\rd$,
$x^\intercal M x\geq 0$. Whereas the matrix $M$ is said to positive definite if and only if $M$ is symmetric for all $x\in\rd$, $x\neq 0$, 
$x^\intercal M x>0$. An equivalent definition relies on eigenvalues (for a square matrix $M$, the complex values $\lambda$ such that $Ax=\lambda x$ for some non zero $x$) and a matrix is positive semidefinite (resp. definite) if and only if all eigenvalues are non-negative (resp. positive); recalling that the eigenvalues of a symmetric matrix are real.
Finally, a matrix $M$ is negative semidefinite (resp. definite) if $-M$ is positive semidefinite (resp. definite).
We will write $M\succeq 0$ (resp. $M\succ 0$)  when $M$ is positive semi-definite (resp. definite).

For a given symmetric matrix $M$ of size $d$ , we will consider the real eigenvalues in descending order   $\lmax{M}=\lambda_1(M)\geq \lambda_2(M)\geq \ldots\geq \lambda_d(M)=\lmin{M}$. Moreover, we denote by $\rho(M)$ the spectral radius of a square matrix $M$ of size $d$ that is the quantity $\max\{|\lambda_i(M)|,\ i=1,\ldots,d\}$ where $|\cdot|$ denotes the modulus of a complex number.

\section{Problem statement}
\label{statement}
In this paper, we are interested in proving automatically some properties on a discrete-time affine system. We start by briefly recalling the notion of affine systems and their reachable values set. Then we present the optimization problem that we have to solve to prove or disprove a property on an affine system.

\subsection{Affine systems}

We can represent the evolution of an affine system by the following relation:
\begin{equation}
\label{context}
x_0\in \xin,\ \forall\, k\in\nn,\ x_{k+1}=A x_k + b  
\end{equation}
where:
\begin{itemize}
\item $A$ is a non-zero square matrix of size $d\times d$;
\item $b$ is vector of $\rd$;
\item  $\xin$ is a non-empty polytope (bounded polyhedra). 
\end{itemize}
We insist on the fact that the initial values are represented by an infinite bounded set. It can be interpreted as a set of perturbations, non-determinism, different cases...

It is well-known that, for all $k\in\nn$, the term of the sequence defined at Equation~\eqref{context} can be expressed as follows:
\[
x_k=A^k x_0+ \sum_{i=0}^{k-1} A^i b,\ x_0\in\xin
\]

From the latter expression of the state-variables, we can define the reachable values set $\rea$ of the affine system presented at Eq.~\eqref{context} :
\begin{equation}
\label{reach}
\rea=\bigcup_{k\in \nn} \left(A^{k}(\xin)+\sum_{i=0}^{k-1} A^i b\right)
\end{equation}
where $A^i$ and $A^k$ denote the number of image iterates of $A$ (powers of matrix $A$) . 

\subsection{The verification problem}
In this paper, we are interested in proving properties on affine systems. We are focusing on the properties supposed to be true for all possible values of the state-variable $x_k$ i.e. for all $k\in\nn$ $x_k$ has to satisfy the property. Since, we can represent a property as to belong to some set $C\subset \rd$, to prove a property is equivalent to prove that, $\rea\subseteq C$. In this paper, we consider the sets $C$  of the form $\{x\in\rd, x^\intercal Q x+q^\intercal x\leq \alpha\}$ where $Q$ is a symmetric matrix of size $d\times d$ and $q\in\rd$. The real number $\alpha$ can be given or proved to be finite (for example to prove the boundedness).  Thus, a verification problem can be viewed as an optimization problem. Indeed, to prove $\rea\subseteq \{x\in\rd, x^\intercal Q x+q^\intercal x\leq \alpha\}$ is equivalent to prove that
$\sup_{x\in\rea} x^\intercal Q x+q^\intercal x\leq \alpha$.
Then to prove the property boils down to compute: 
\begin{equation}
\label{pbopt}
\begin{array}{ll}
&\displaystyle{\sup_{x\in\rea} x^\intercal Q x+q^\intercal x}\\
=&\displaystyle{\sup_{k\in\nn}\sup_{x_0\in\xin}  \left(A^k x_0+ \sum_{i=0}^{k-1} A^i b \right) Q \left( A^k x_0+ \sum_{i=0}^{k-1} A^i b\right) +q^\intercal \left(A^k x_0+ \sum_{i=0}^{k-1} A^i b\right)}\enspace .
\end{array}
\end{equation} 

If the exact optimal value of Problem~\eqref{pbopt} can be computed then we can prove or disprove a property whereas an over-approximation can only prove a property: a too loose over-approximation i.e. greater than $\alpha$ does not imply that the property is false. 

The problem in the computation of the optimal value of Problem~\eqref{pbopt} resides, first, in the infinite number of quadratic optimization problems that we have to solve. In a second time, the quadratic optimization problems are non-necessarily concave and thus can be difficult to solve. 
\section{Results}
\label{mainresults}
This section contains the main contribution of the paper i.e. the computation of the optimal value of Problem~\eqref{pbopt}. This latter computation uses a discretization of initial values set and the computation of a finite integer after which we know that the optimal cannot be reached. In this section, we begin to recall some basic results that we need in our development. We then study the case of linear systems i.e. where $b=0$ in Eq.~\eqref{context}. Finally, we show how to use the linear case to solve the case where the system is affine.   
\subsection{Basic notions}

For a given square matrix $M$, the matrix $P$ satisfies the discrete Lyapunov equation if and only $P\succ 0$ and $P-M^\intercal P M\succeq 0$. If $\rho(M)<1$, then the discrete Lyapunov equation has at least one solution. Moreover, in this case, we can find $P\succ 0$ such that $P-M^\intercal P M\succ 0$.  

For the matrix $A$ of the affine system considered at Eq.~\eqref{context}, we introduce the set $\lyap{A}$ of the  solutions  of the discrete Lyapunov equation :
\[
\lyap{A}=\{P\in\sym \mid P\succ 0,\ P-A^\intercal P A\succ 0\}\enspace .
\]
Hence, $\rho(A)<1$ ensures that $\lyap{A}$ is non-empty. More precisely, $\rho(A)<1$ if and only if for all positive definite matrices $R$ there exists a unique positive definite matrix $P$ such that $P-A^\intercal P A=R$. \ForAuthors{reference sur stein equation}

To prove some results, we need to recall some basic results presented as lemmas. The first one involves a double inequality between quadratic forms and Euclidean norm. When the quadratic form is positive definite then the double inequality proves the norm equivalence between the norm defined by the quadratic form and the Euclidean norm with explicit constants. 
\begin{lemma}
\label{lemma1}
Let $M$ be in $\sym$. We recall that $\lmax{M}$ (resp. $\lmin{M}$) denotes the greatest eigenvalue of $M$ (resp. the smallest eigenvalue of $M$) then, for all $x\in \rd$:
\begin{equation}
\label{eigineg}
\lmin{M}\norm{x}_2^2\leq x^\intercal M x\leq \lmax{M} \norm{x}_2^2
\end{equation}
where $\norm{\cdot}_2$ is the Euclidean norm. 
\end{lemma}
Inequalities~\eqref{eigineg} can be formulated in term of positive semi-definiteness:
\[
\lmin{M}\Idd\preceq M\preceq \lmax{M}\Idd
\]

We recall Weyl's inequalities~\cite{horn1990matrix} that provide inequalities for the eigenvalues of the sum of two symmetric matrices and the sum of the eigenvalues of each matrix.  
\begin{lemma}[Weyl's inequalities]
\label{le}
Let $M$ and $N$ be symmetric matrices. We have, for all $\ell\in\{1,\ldots,d\}$ :
\[
\lambda_\ell(M)+\lmin{N}\leq \lambda_\ell(M+N)\leq \lambda_\ell(M)+\lmax{N}
\]
\end{lemma}
The following lemma recalls that, when we maximize a convex function over a polytope, it suffices to consider the values of the function at the extreme points. 
\begin{lemma}[Maximisation of a convex function over a polytope]
\label{lemma2}
Let $C$ be a polytope and $f:\rd\to\rd$ a convex function. Then:
\[
\max_{x\in C} f(x)=\max_{x\in \mathcal{E}(C)} f(x)
\]
where $\mathcal{E}(C)$ denotes the finite set of the extreme points (vertices) of $C$.
\end{lemma}
Note that the result also holds for convex compacts sets.  
\subsection{Verification Linear Time-Invariant Discrete-time Systems}
\label{mainsub}
Now we come back to the verification problem presented at~\eqref{pbopt}. In this subsection, we suppose that the system in linear i.e. $b=0$ in~\eqref{context}. We will show at Subsection~\ref{affine} that the case $b\neq 0$ can be reduced to the linear case.

Since $b=0$, Problem~\eqref{pbopt} becomes :
\[
\sup_{k\in\nn}\sup_{x_0\in\xin} x_0^\intercal (A^k)^\intercal Q A^k x_0+q^\intercal A^k x_0\enspace .
\]

The main goal of this subsection is to {\bf compute the smallest possible integer} $\mathbf K$ such that:
\[
 \sup_{x\in\rea} x^\intercal Q x+p^\intercal x=\sup_{k\in\{0,\ldots,\mathbf K\}}\sup_{x_0\in\xin} x_0^\intercal (A^k)^\intercal Q A^k x_0+q^\intercal A^k x_0\enspace .
\]

Without any assumption on $Q$, the computation of  $\sup_{x_0\in\xin} x_0^\intercal (A^k)^\intercal Q A^k x_0+q^\intercal A^k x_0$ for all $k\in\{0,\ldots,K\}$ is still difficult. Then we make the following assumption : 
\begin{assumption}
\label{assum1}
The matrix $Q$ is non-null and positive semi-definite.
\end{assumption}

From Assumption~\ref{assum1}, Problem~\eqref{pbopt} has a finite optimal value if either $\rea$ is bounded or for all 
$x_0\in\xin$ that generates an unbounded sequence, the sequence $x_k=A^k x_0$ lies eventually in the null space of $Q$ ($x_k^\intercal Q x_k=0$ for sufficiently large integers $k$) and satisfies eventually $q^\intercal A^k x_0\leq \beta$ for some $\beta\in\rr$ .  We will consider bounded reachable values set. This latter restriction will not be enough for our development. Using Jordan block decompositions, the reachable values set can be bounded even if the spectral radius of $A$ is equal to 1. The construction of our $K$ depends on the fact that there exists a matrix norm $N$ such that $N(A)<1$. This condition is not compatible with the case where the spectral radius of $A$ is equal to 1.

\begin{assumption}
\label{assum2}
The spectral radius $\rho(A)$ of $A$ is strictly smaller that 1.
\end{assumption}

Assumption~\ref{assum2} ensures that $\lyap{A}\neq \emptyset$. Since for all $P\in\lyap{A}$, $P\succ 0$, we can define a norm on $\rd$ associated with $P\in\lyap{A}$ as follows: for all $x\in\rd$:
\[\norm{x}_P:=\sqrt{x^\intercal P x}\enspace. \]
For any norm $\norm{\cdot}$, we can define the operator norm as $\norm{A}=\max_{x\neq 0} x^\intercal A x/ x^\intercal x$. In the case where the norm is constructed from a positive definite matrix, we can compute $\norm{A}_P$ as follows:
\begin{equation}
\label{normdef}
    \norm{A}_P^2=\max_{x\neq 0} \dfrac{x^\intercal A^\intercal P A x}{x^\intercal P x}=\inf\{\alpha>0\mid \alpha P-A^\intercal P A\succeq 0\}
\end{equation}

Let $t>0$. We introduce the following set :  
\[
\conq(t)=\{P\in\sym\mid tP-Q\succeq 0\}
\]
Since, under Assumption~\ref{assum1}, $P\in\conq(t)$ implies that $P$ is positive semi-definite, then $\conq(t)$ is a pointed convex cone. Moreover, for all positive definite matrix $P$, there exists $t>0$ such that $P\in \conq(t)$. 

Then, we introduce a related set, which is parameterized by $P\in \sym$.
\[
\cont(P)=\{t>0\mid P\in\conq(t) \}=\{t>0\mid tP-Q\succeq 0\}
\]

Let us introduce the family $\lyat{A,Q}$ parameterized by positive numbers $t$ of sets of solutions of discrete Lyapunov equations defined as follows:
\[
\lyat{A,Q}=\conq(t)\cap\lyap{A}=\{P\succ 0\mid tP-Q\succeq 0,\ P-A^\intercal P A\succ 0\}\enspace. 
\]
Note that, since the set of positive definite matrix is a cone, we get the following equivalence :
\begin{equation}
\label{simpleequiv}
P\in\lyat{A,Q}\iff tP\in\lyaun\enspace.
\end{equation}
 
Actually we can bound $\cont(P)$ from below. Since $P$ is positive definite, we can use the inverse of the square root $P^{-1/2}$ of $P$ . We recall that this matrix is defined from the spectral decomposition of $P=U D U^\intercal$ where $D$ is a diagonal  matrix of ordered (strictly positive) eigenvalues $\lambda_k(P)$ and $U$ is an orthogonal matrix i.e. $U U^\intercal = U^\intercal U=\Idd$.   The matrix  $P^{-1/2}$ is thus defined by 
$U D^{-1/2} U^\intercal$ where $D^{-1/2}$ is the diagonal matrix whose the diagonal elements are equal $\lambda_k(P)^{-1/2}$. This easy to see that $P^{-1/2} P^{-1/2}=P^{-1}$. 

\begin{prop}
\label{propcont}
For all $P\succ 0$, for all $t\in\cont(P)$, $t\geq \lmax{P^{-1/2} Q P^{-1/2}}$.
\end{prop}
\begin{proof}
Let $P\succ 0$, let $t\in\cont(P)$. Let $x\in\rd$, $x\neq 0$. We have 
$x^\intercal Q x\leq t x^\intercal P x$ and thus $(x^\intercal Q x)(x^\intercal P x)^{-1}\leq t$. Finally:
$\sup_{x\neq 0}(x^\intercal Q x)(x^\intercal P x)^{-1}\leq t $. Writing $y=P^{1/2}x$ ( recall that $P^{1/2}$ is invertible as $P\succ 0$), we obtain :
$\sup_{y\neq 0}(y^\intercal P^{-1/2}Q P^{-1/2}y)(y^\intercal y)^{-1}\leq t $. However, it is well-know that
$\sup_{z\neq 0} (z^\intercal M z)(z^\intercal z)^{-1}=\lmax{M}$. In fact, $\lmax{M}$ is achieved for an eigenvector associated with $\lmax{M}$. Hence, 
$\sup_{y\neq 0} (y^\intercal P^{-1/2}Q P^{-1/2}y)(y^\intercal y)^{-1}=\lmax{P^{-1/2} Q P^{-1/2}}$. Finally $\lmax{P^{-1/2} Q P^{-1/2}}P-Q\succeq 0$ is a direct consequence of Lemma~\ref{lemma1}. 

\end{proof}
To compute our $\bigk$, we introduce a new notation and a new assumption. 
 \begin{assumption}
\label{assumR}
There exists $k\in\nn$ such that\[\min\left\{\sup_{x\in\xin} x^\intercal Q x , \sup_{x\in\xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\right\}\] is strictly positive.
\end{assumption}

Note that Assumption~\ref{assumR} implies that $\sup_{x\in\nn} x^\intercal Q x$ cannot be null. Even if for some $k$, the value
$\sup_{x\in\xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x$ is strictly positive, the minimal value would be equal to 0. 
Assumption~\ref{assumR} can be written as there exists  some $k\in\nn$, for which $\sup_{x\in\xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x$ is strictly positive and $\sup_{x\in\xin} x^\intercal Q x$ is strictly positive.

We will need the following notation :
\[
\ks:=\inf\left\{k\in\nn\mid \min\left\{\sup_{x\in\xin} x^\intercal Q x , \sup_{x\in\xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\right\}>0\right\}
\]
and
\[
\lfrak:= \min\left\{\sup_{x\in\xin} x^\intercal Q x , \sup_{x\in\xin} x^\intercal (A^{\ks})^\intercal Q A^{\ks}k x+q^\intercal A^{\ks}x\right\}
\]

Note that Assumption~\ref{assum1} is not sufficient to ensure the validity of Assumption~\ref{assumR}. The matrix $Q$ is only supposed to be positive semi-definite and thus $\sup_{x\in\xin} x^\intercal Q x$ can be null. Moreover, $\lfrak$ can be negative since $q^\intercal A^k x$ can be negative on $\xin$. Strongest assumptions can be made : the global positivity of $x^\intercal (A^k)^\intercal  Q A^k x+q^\intercal A^k x$ which is equivalent to check whether a matrix is definite positive or a copositivity condition relying $Q,q$ and $\xin$. 

Now, we define the key numbers that we need to solve the verification problem. We explain latter in the proof how they are constructed. Moreover, we will study later on numerical examples how much they are accurate.  


Let $t>0$ and $P\in\lyap{A}$ such that $P\in \lyat{A,Q}$. Let us define the following formula :
\begin{equation}
\label{intgfond}
K(t,P)=\left\lfloor \dfrac{\ln\left[\left(\sqr{\lfrak+\vqQ(t,P)^2}-\vqQ(t,P)\right)\left(\sqr{t}\mu(P)\right)^{-1}\right]}{\ln\left(\norm{A}_P\right)}\right\rfloor+1
\end{equation}
where 
\[
\vqQ(t,P):=\dfrac{\norm{q}_2 }{2\sqr{t\lmin{P}}}\quad \text{ and } \mu(P)=\sup_{x\in\xin} \sqr{x^\intercal P x}
\]


\begin{prop}
For all $t>0$, $P\in \lyap{A}$ such that $P\in \lyat{A,Q}$, $1\leq K(t,P)< +\infty$.  
\end{prop}

It suffices to show that both numerator and denominator are negative and finite. Since we use the natural logarithm, this is equivalent to show that the arguments of the natural logarithm of the numerators lie in the interval $(0,1]$ and $\norm{A}_P$ lies in the interval $(0,1)$ for all $P\in\lyap{A}$. 

\begin{lemma}
\label{lyapnorm}
For all $P\in\lyap{A}$, $0<\norm{A}_P<1$. 
\end{lemma}

\begin{proof}
Let $P$ such that $P\succ 0$ and $P-A^\intercal P A\succ 0$.  First, the matrix $A$ is not null then its spectral radius is strictly positive. Then since $\norm{\cdot}_P$ is a matrix norm, $0<\rho(A) \leq \norm{A}_P$.

Secondly, since $P-A^\intercal P A\succ 0$ and using Eq.~\eqref{normdef}, we have $\norm{A}_P\leq 1$. Still using Eq.~\eqref{normdef}, to prove $\norm{A}_P<1$, it suffices to exhibit $\varepsilon>0$ such that $(1-\varepsilon) P-A^\intercal P A\succeq 0$ with $0<\varepsilon<1$. 

Let us write $\varepsilon=\lmin{P-A^\intercal P A} \lmax{P}^{-1}$. Let us prove that $0<\varepsilon<1$ and $(1-\varepsilon) P-A^\intercal P A\succeq 0$.  Since $P-A^\intercal P A\succ 0$, $\lmin{P-A^\intercal P A}>0$ and from $P\succ 0$ we conclude that $\varepsilon >0$. Since $P=P-A^ \intercal P A +A^\intercal P A$, we have, from Weyl's inequalities, $\lmin{P-A^\intercal P A}+\lmax{A^\intercal P A}\leq \lmax{P}$. Now, $A^\intercal P A\succeq 0$ from $P\succ 0$ and then $\lmax{A^\intercal P A}\geq 0$. We conclude that $\varepsilon\leq 1$. Now, if $\lmin{P-A^\intercal P A}=\lmax{P}$ i.e. $\varepsilon=1$, we have 
$\lmax{A^\intercal P A}=0$ and then $x^\intercal A^\intercal P A x=0$ for all $x\in \rd$ and $A$ is the null matrix. 
Finally, $\varepsilon<1$. 

Now from Lemma~\ref{lemma1}, $\varepsilon P=P\lmax{P}^{-1}\lmin{P-A^\intercal P A}\preceq \lmin{P-A^\intercal P A}Id_d\preceq 
P-A^\intercal P A$ which implies that $(1-\varepsilon) P -A^\intercal P A\succeq 0$. 
\end{proof}

Now we prove that the argument of the natural logarithm at the numerator appearing in $K(t,P)$ lies in $(0,1]$. 

\begin{lemma}
For all $t>0$ and $P\in\lyap{A}$ such that $P\in\lyat{A,Q}$: \[
\left(\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)\right) t^{-1/2}\mu(P)^{-1}\in (0,1]\enspace .
\]
\end{lemma}

\begin{proof}
First $t>0$ and $P\succ 0$ then $t^{-1/2}\mu(P)^{-1}>0$ and $\vqQ(t,P)\geq 0$. Using Assumption~\ref{assumR}, we get $(\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)> \vqQ(t,P)-\vqQ(t,P)=0$. We conclude that $\left((\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)\right) t^{-1/2}\mu(P)^{-1}>0$. 

Now, since $\lfrak$, $\vqQ(t,P)$ are non-negative and for all $a,b\geq 0$, $\sqrt{a+b}\leq \sqrt{a}+\sqrt{b}$  then : 
$(\lfrak+\vqQ(t,P)^2)^{1/2}\leq \lfrak^{1/2}+\vqQ(t,P)$. Hence : 
\[
\begin{array}{lll}
&\left((\lfrak+\vqQ(t,P)^2)^{1/2}-\vqQ(t,P)\right) t^{-1/2}\mu(P)^{-1}& \\
\leq &\lfrak^{1/2} t^{-1/2}\mu(P)^{-1} & \\
\leq &(\displaystyle{\sup_{x\in\xin} x^\intercal Q x})^{1/2} t^{-1/2}\mu(P)^{-1} & \text{ from the def. of } \lfrak
\end{array}
\]

Since $t>0$ and $P\in\lyap{A}$ satisfy $P\in\lyat{A,Q}$, then $t P-Q\succeq 0$. It follows that 
$t x^\intercal P x\geq x^\intercal Q x$ for all $x\in \xin$. Taking the supremum over $\xin$ leads to $(\sup_{x\in\xin} x^\intercal Q x)\mu(P)^{-2}t^{-1} \leq 1$. 
\end{proof}

\begin{prop}
The following assertions are true:
\begin{itemize}
\item For all $P\in\lyap{A}$, the function defined on $\cont(P)$, $t\mapsto K(t,P)$ is increasing. Then : 
\begin{equation}
\label{infsolved}
\inf_{P\in\lyap{A}} \inf_{t\in\cont(P)} K(t,P)=\inf_{P\in\lyap{A}}K(\lmax{P^{-1/2} Q P^{-1/2}},P)
\end{equation}
\item For all $t>0$, for all $P\in\lyat{A,Q}$, $K(t,P)=K(1,tP)$ and thus:
\[
\inf_{t>0} \inf_{P\in\lyat{A,Q}} K(t,P)=\inf_{P\in\lyaun} K(1,P)
\]
\end{itemize}
\end{prop}

\begin{proof}
Let us prove the first assertion.
Let $P\in\lyap{A}$. Let $t\in\cont(P)$. The integer part is increasing then we have to prove that the argument of the integer part is increasing in $t$. From Lemma~\ref{lemma2}, we know that $\ln{\norm{A}_P}<0$. Hence, since the natural logarithm is increasing it suffices to show that $\varphi:t:\mapsto \left(\sqr{\lfrak+\vqQ(t,P)^2}-\vqQ(t,P)\right)\left(\sqr{t}\mu(P)\right)^{-1}$ is decreasing. However :
\[
\begin{array}{lll}
\varphi(t)&=&\dfrac{\lfrak}{\left(\sqr{\lfrak+\vqQ(t,P)^2}+\vqQ(t,P)\right)\sqr{t}\mu(P)}\\
\\
&=&\dfrac{\lfrak}{\left(\sqr{\lfrak+\dfrac{\norm{q}_2^2 }{4t\lmin{P}}}+\dfrac{\norm{q}_2 }{2\sqr{t\lmin{P}}}\right)\sqr{t}\mu(P)}\\
\\
&=&\dfrac{\lfrak}{\left(\sqr{t\lfrak+\dfrac{\norm{q}_2^2 }{4\lmin{P}}}+\dfrac{\norm{q}_2 }{2\sqr{\lmin{P}}}\right)\mu(P)}
\end{array}
\]
We conclude that $t\mapsto \varphi(t)$ is decreasing as an inverse of an increasing function. 
Finally, Eq.~\eqref{infsolved} follows from Prop.~\ref{propcont} and $t\mapsto K(t,P)$ is increasing for all $P\in\lyap{A}$.  

Now, let $t>0$ and $P\in\lyat{A,Q}$. To prove $K(t,P)=K(1,tP)$ we have to show that $\vqQ(t,P)=\vqQ(1,tP)$, $\sqrt{t}\mu(P)=\mu(tP)$
and $\norm{A}_{tP}=\norm{A}_P$. The fact $\vqQ(t,P)=\vqQ(1,tP)$ comes from the fact that $\lmin{tP}=t\lmin{P}$ since $t>0$. Second the supremum is homogeneous and then $\sqrt{t}\mu(P)=\mu(tP)$. Finally, $\norm{A}_{tP}=\norm{A}_P$ is a direct consequence of the definition given at Eq.~\eqref{normdef}. We conclude that :
\[
\begin{array}{llll}
\displaystyle{\inf_{t>0} \inf_{P\in\lyat{A,Q}} K(t,P)}&=&\displaystyle{\inf_{t>0} \inf_{P\in\lyat{A,Q}} K(1,tP)}& \text{ from } K(t,P)=K(1,tP)\\
&=&\displaystyle{\inf_{t>0} \inf_{tP\in\lyaun} K(1,tP)}&\text{ from Eq.~\eqref{simpleequiv}} \\ 
&=&\displaystyle{\inf_{t>0} \inf_{P\in\lyaun} K(1,P)}&\\
&=&\displaystyle{\inf_{P\in\lyaun} K(1,P)}&
\end{array}
\]
\end{proof}

%\begin{prop}
%Suppose that $q=0$ and let $P\in\lyaun$. Then: \[K(1,P)=1\iff\sup_{x\in \xin} x^\intercal P x=\sup_{x\in\xin} x^\intercal Q x\enspace.\] 
%\end{prop}

We now present the main result of the paper.
\begin{theorem}
\label{thfond}
Let us define \[\bigk=\min\left\{\inf_{P\in\lyaun} K(1,P),\inf_{P\in\lyap{A}} K(\lmax{P^{-1/2}QP^{-1/2}},P)\right\}\enspace .\] Let $k\geq \bigk$. Then : 
\[
\max_{x\in \xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\leq \lfrak
\]
\end{theorem}

\begin{proof}
Let $t>0$ and $P\in\lyap{A}$ such that $P\in\lyat{A,Q}$. Let also $k\in\nn$ and $x\in\xin$. We have:
\[
x^\intercal (A^k)^\intercal Q A^k x  
\leq  t \norm{A^k x}_P^2\leq t\norm{A}^{2k}_P \norm{x}^2_P
\leq t\norm{A}_P^{2k} \mu(P)^2
\]
The first inequality comes from the definition of $\lyat{A,Q}$, the second from the matrix norm definition and the last from the definition of $\mu(P)$ (see Eq.~\eqref{intgfond}).

We also have, for all $x\in\xin$ :
\[
q^\intercal A^k x
\leq \norm{q}_2 \norm{A^k x}_2\leq \dfrac{\norm{q}_2 \norm{A^k x}_P}{\sqr{\lmin{P}}} 
\leq \dfrac{\norm{q}_2\norm{A}_P^{k} \mu(P)}{\sqr{\lmin{P}}}
\]
The first inequality comes from Cauchy-Schwarz, the second from Lemma~\ref{lemma1} and the last from the matrix norm definition and the definition of $\mu(P)$. 

Summing the two parts leads to :
\[
x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\leq \left(\sqrt{t}\norm{A}_P^k\mu(P)+\vqQ(t,P)\right)^2-\vqQ(t,P)^2
\]
where $\vqQ$ is defined at Eq.~\eqref{intgfond}. Then, $ \left(\sqrt{t}\norm{A}_P^k\mu(P)+\vqQ(t,P)\right)^2-\vqQ(t,P)^2\leq \lfrak$ implies that $ \max_{x\in \xin} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x\leq \lfrak$. Now, we remark that  $\left(\sqrt{t}\norm{A}_P^k\mu(P)+\vqQ(t,P)\right)^2-\vqQ(t,P)^2\leq \lfrak$ is equivalent to :
\[
\norm{A}_P^k\leq \left(\sqr{\lfrak+\vqQ(t,P)^2}-\vqQ(t,P)\right)\left(\sqr{t}\mu(P)\right)^{-1}
\]
Using the natural logarithm and Lemma~\ref{lyapnorm}, the condition  $k\geq K(t,P)$ is sufficient for the latter inequality .  The latter proof is valid for all couple $t>0$ and $P\in\lyap{A}$ such that $P\in\lyat{A,Q}$. So it remains true for   $\bt>0$ and $\bP\in\lyap{A}$ such that $P\in\lyat{A,Q}$ and $\bigk=K(\bt,\bP)$. 
\end{proof}
We recall that $\lfrak$ is smaller than $\max_{x\in\xin} x^\intercal {A^{\ks}}^\intercal Q A^{\ks} x+q^\intercal A^{\ks} x$. Hence, the optimal value of Problem~\ref{pbopt} is greater than $\lfrak$ since $A^{\ks} (\xin)\subseteq \rea$. We conclude that, following Th.~\ref{thfond}, the optimal value of Problem~\ref{pbopt} is attained for powers of $A$ between $\ks$  and $\max\{\bigk,\ks\}$.

\begin{corollary}
The following statement is true :
 \[
\max_{y\in\rea} y^\intercal Q y+q^\intercal y=\max_{\ks\leq k\leq \max\{\bigk-1,\ks\}} \max_{x\in \mathcal{E}(\xin)} x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x
\]
\end{corollary}

\begin{proof}
Let $k\in\nn$. We define :
\[
\nu_k:x\mapsto x^\intercal (A^k)^\intercal Q A^k x+q^\intercal A^k x 
\]
Using extreme points instead of the whole set $\xin$ is a direct consequence of the convexity of the functions $\nu_k$ (Assumption~\ref{assum1}) and Lemma~\ref{lemma2}.

Now we write $k^*=\max\{\bigk-1,\ks\}$. We have :
 \[
\max_{y\in\rea} y^\intercal Q y+q^\intercal y
=\max_{x\in \mathcal{E}(\xin)}\max\left\{\max_{0\leq k\leq \ks} \nu_k(x), \max_{\ks \leq k\leq k^*} \nu_k(x),\max_{k>k^*} \nu_k(x)\right\}
\]

%&=&\displaystyle{\max\left\{\max_{0\leq k\leq \bigk-1}\max_{x\in \mathcal{E}(\xin)} \nu_k(x),\lfrak\right\}}
%\end{array}
%\]
Now, for all $k< \ks$, for all $x\in\xin$, $\nu_k(x)\leq 0$ and for all $x\in\xin$, $\nu_{\ks}(x)\leq \lfrak$. Moreover, from Th~\ref{thfond}, 
for all $k>k^*$, $\nu_k(x)\leq \lfrak$. The conclusion follows from $\displaystyle{\max_{x\in\xin} \max_{\ks\leq k\leq k^*} \nu_k(x)\geq \max_{x\in\xin} \nu_{\ks}(x)\geq \lfrak}$. 
%The conclusion follows from the fact that  $\lfrak\leq \max_{x\in \mathcal{E}(\xin)} \nu_{\ks}(x)$.
\end{proof}
The integer $\bigk$ is the best integer than we can get in theory. In practice, we cannot compute this integer since we cannot solve the minimization problems :
\[
\inf_{P\in\lyaun} K(1,P)\text{ and }\inf_{P\in\lyap{A}}K(\lmax{P^{-1/2} Q P^{-1/2}},P)
\]
In practice, for the moment, we only compute two integers : $K( 1,P)$ where $P$ is an optimal solution of a perturbation of the minimization problem : 
\begin{equation}
\label{minlmaxQ}
\inf\{\lmax{P}\mid P\in \lyaun\}
\end{equation}
and $K(\lmax{P^{-1/2} Q P^{-1/2}},P)$ where $P$ is an optimal solution of a perturbation of the minimization problem :
\begin{equation}
\label{minlyap}
\inf\{\lmax{P}\mid P\in\lyap{A}\}\enspace .
\end{equation}
Then we take the minimum of the  two integers. We come back latter on the perturbation of the two minimization problems at Section~\ref{computations}.
\subsection{From linear systems to affine ones}
\label{affine}
We come back to Problem~\eqref{context}: the case where the system is affine ($b\neq 0$). We recall that we have :
%Let us consider the same problem where we replace the linear dynamics by an affine one:
\[
x_0\in \xin,\ \forall\, k\in\nn,\ x_{k+1}=A x_k+b,\
\]
where $A$ is a $d\times d$ matrix and $b$ a vector of $\rd$. We recall that $\rea$ is given by Formula~\eqref{reach} and we consider the optimization problem:
\[
\sup_{x\in\rea} x^\intercal Q x+q^\intercal x
\]
Assumption~\ref{assum2} still holds and it implies that $Id-A$ is invertible. It is well-known that the sequence defined by :
\[
\forall\, k\in\nn,\ y_k=x_k-B,\text{ where } B=(Id-A)^{-1} b
\]
satisfies:
\[
\forall\, k\in\nn,\ y_{k+1}=A y_k. 
\]
Finally, we have :
\[
\forall\, k\in\nn,\ x_k=A^k y_0+B
\]
This latter expression leads to a new formulation of Problem~\eqref{pbopt}: 
\[
\sup_{k\in\nn} \sup_{y_0\in\xin-B} \left(A^k y_0+B\right)^\intercal Q  \left(A^k y_0+B\right)+q^\intercal  \left(A^k y_0+B\right)
\]
or
\[
\sup_{k\in\nn} \sup_{y_0\in\xin-B} y_0^\intercal (A^k)^\intercal Q  A^k y_0+(2B+q)^\intercal A^k +B^\intercal Q B+q^\intercal B
\]
We conclude that we can the results developed in Subsection~\ref{mainsub} where the matrix $Q$ is unchanged and the vector $q$ to use is now $2B+q$. The polytope of initial conditions also changes since we have to consider now $\xin-B$. We have to adapt Assumption~\ref{assumR} with  and the new  $2B+q'$ and $\xin-B$.

\section{Practical computations of $K(t,P)$}
\label{computations}
The computations have been performed using Matlab. We use classical internal routines for powers of matrices  (needed for $A^{\ks}$) and  the vector Euclidean norm (needed for $\norm{q}_2$). We discuss here the computations that rely on semi-definite programming. The solver used in the development is Mosek interfaced with Yalmip. 

In the computations of the two integers, we need first a matrix that satisfies the discrete Lyapunov equation. For the computation of $K(1,P)$, we need $P \in \lyaun$. We propose to compute this matrix $P$ as an optimal solution  of the semi-definite problem:
\begin{equation}
\label{eq:kunp}
\begin{array}{llc}
\Min & &\lmax{P}\\
 & \st &\left\{\begin{array}{l} 
         P-Q\succeq 0\\
         P-A^\intercal P A-\varepsilon Id\succeq 0\\
         P\succeq  0
        \end{array}\right.
\end{array}
\end{equation}
where $\varepsilon$ is set to $0.001$.

For the computation of $K(\lmax{P^{-1/2}QP^{-1/2}},P)$, we need an element of $\lyap{A}$. We then solve the semi-definite problem:
\begin{equation}
\label{eq:klmaxp}
\begin{array}{llc}
\Min & &\lmax{P}\\
 & \st &\left\{\begin{array}{l} 
         P-A^\intercal P A-\varepsilon Id\succeq 0\\
         P\succeq  0
        \end{array}\right.
\end{array}
\end{equation}
where $\varepsilon$ is still set to $0.001$.
We will write $P_1$ for an optimal solution of Pb.~\eqref{eq:kunp} and $\pmax$ for an optimal solution of Pb.~\eqref{eq:klmaxp}. 

Once $\pmax$ computed, we have to compute $\lmax{\pmax^{-1/2}Q \pmax^{-1/2}}$ which can be done by semi-definite programming:
\[
\lmax{\pmax^{-1/2}Q \pmax^{-1/2}}=\inf\{\alpha>0\mid \alpha \pmax-Q\succeq 0\}
\] 

Now, for both $P_1$ and $\pmax$ we have to compute $\lmin{\cdot}$, $\mu(\cdot)$ and $\norm{A}_{\cdot}$. Folowing Lemma~\ref{lemma1}, since $P_1$ and $\pmax$ are symmetric, the minimal eigenvalue
of those matrices can be computed from semi-definite programming. Indeed for all symmetric matrix $M$, we have : 
\[
\lmin{M}=\inf\{\alpha>0\mid M-\alpha Id\geq 0\}
\] 
To compute $\mu(P_1)$ and $\mu(\pmax)$ we suppose that number of vertices of $\xin$ is small and we evaluate directly $\mu(\cdot)$ by enumeration of the image of vertices of $\xin$ and choose the maximal value in the finite list. In a future work, we consider scalable techniques to compute the exact value of $\mu(\cdot)$. The square root (as it is increasing) involved $\mu(\cdot)$ can be performed at the end of the maximization process: we compute the values $x^\intercal P_1 x$ (resp. $x^\intercal \pmax x$) for all vertices; take the maximum of all of them and finally take the square root of the maximal value. 

Finally, following Eq.~\eqref{normdef}, the value $\norm{A}_\cdot$ is compute from semi-definite programming; taking the square root of the optimal value.

To complete the computations, we have to deal with $\lfrak$. We suppose that the integer $\ks$ is given and then to compute $\lfrak$ we employ the same technique used for $\mu(\cdot)$. It suffices to browse the vertices of $\xin$ once since we evaluate, for each vertice, $\min\{x^\intercal Q x, x^\intercal {A^{\ks}}^\intercal Q {A^{\ks}}x+q^\intercal A^{\ks} x\}$ and take the maximal value. 
\section{Experiments}
\label{experiments}
We decompose the experiments into two sub-classes: the linear and affine systems. Those sub-classes are separeted in three parts : the  case where the objective function is a quadratic homogeneous function; the linear case and the non-homogeneous quadratic objective function. For each cut, we provide a numerical example.  
\subsection{Linear systems}

\subsubsection{Homogeneous objective function}

\subsubsection{Non-Homogeneous objective function}

\subsubsection{Random matrices}

\subsection{Affine systems}

\subsubsection{Homogeneous objective function}

\subsubsection{Non-Homogeneous objective function}

\subsubsection{Random matrices}
\section{Conclusion and Future Works}
\label{conclusion}
In this paper, we introduce a new class of optimization problems. This class solves verification problem coming from computer science. This class can be described as follows : the objective function is convex and quadratic and the decision variable is constrained to belong to the reachable values set of a discrete-time affine system which is not a convex set. The main issue is the infinite horizon problem. To maximize the function we can wait for an arbitrary long time. Thus the paper proposes to compute, in the case of stable affine system, a finite horizon after which the search of an optimal value is useless. The problem becomes a finite horizon problem which can be solved in a finite time. 

Nevertheless some computational questions are left open. In this paper, we describe the technique on small examples and for which the number of vertices of the initial polyhedron is small. A more scalable approach can be done in a future work using known algorithms to solve the maximization of convex quadratic functions over linear inequalities. 

The most  difficult part resides in the minimization of the horizons with respect to Lyapunov functions. The non-convexity of the function makes difficult the minimization procedure.         
\bibliographystyle{alpha}
\bibliography{paperbib} 
\end{document}
